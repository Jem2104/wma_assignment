{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('memphis_restaurants.csv')\n",
    "df = df.drop(['Unnamed: 0'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Second Line https://www.yelp.com/biz/the-second-line-memphis\n",
      "The Second Line review page 0 has been scraped\n",
      "The Second Line review page 1 has been scraped\n",
      "The Second Line review page 2 has been scraped\n",
      "The Second Line has been scraped\n",
      "Bounty On Broad https://www.yelp.com/biz/bounty-on-broad-memphis\n",
      "Bounty On Broad review page 0 has been scraped\n",
      "Bounty On Broad review page 1 has been scraped\n",
      "Bounty On Broad review page 2 has been scraped\n",
      "Bounty On Broad has been scraped\n",
      "Curfew https://www.yelp.com/biz/curfew-memphis\n",
      "Curfew review page 0 has been scraped\n",
      "Curfew review page 1 has been scraped\n",
      "Curfew review page 2 has been scraped\n",
      "Curfew has been scraped\n",
      "The Liquor Store https://www.yelp.com/biz/the-liquor-store-memphis-3\n",
      "The Liquor Store review page 0 has been scraped\n",
      "The Liquor Store review page 1 has been scraped\n",
      "The Liquor Store review page 2 has been scraped\n",
      "The Liquor Store has been scraped\n",
      "Beauty Shop Restaurant https://www.yelp.com/biz/beauty-shop-restaurant-memphis-3\n",
      "Beauty Shop Restaurant review page 0 has been scraped\n",
      "Beauty Shop Restaurant review page 1 has been scraped\n",
      "Beauty Shop Restaurant review page 2 has been scraped\n",
      "Beauty Shop Restaurant has been scraped\n",
      "Saltwater Crab https://www.yelp.com/biz/saltwater-crab-memphis\n",
      "Saltwater Crab review page 0 has been scraped\n",
      "Saltwater Crab review page 1 has been scraped\n",
      "Saltwater Crab review page 2 has been scraped\n",
      "Saltwater Crab has been scraped\n",
      "Porch and Parlor https://www.yelp.com/biz/porch-and-parlor-memphis\n",
      "Porch and Parlor review page 0 has been scraped\n",
      "Porch and Parlor review page 1 has been scraped\n",
      "Porch and Parlor review page 2 has been scraped\n",
      "Porch and Parlor has been scraped\n",
      "Maciel’s Tortas & Tacos https://www.yelp.com/biz/maciels-tortas-and-tacos-memphis-7\n",
      "Maciel’s Tortas & Tacos review page 0 has been scraped\n",
      "Maciel’s Tortas & Tacos review page 1 has been scraped\n",
      "Maciel’s Tortas & Tacos review page 2 has been scraped\n",
      "Maciel’s Tortas & Tacos has been scraped\n",
      "Sweet Grass https://www.yelp.com/biz/sweet-grass-memphis\n",
      "Sweet Grass review page 0 has been scraped\n",
      "Sweet Grass review page 1 has been scraped\n",
      "Sweet Grass review page 2 has been scraped\n",
      "Sweet Grass has been scraped\n",
      "Tamboli’s Pasta & Pizza https://www.yelp.com/biz/tambolis-pasta-and-pizza-memphis\n",
      "Tamboli’s Pasta & Pizza review page 0 has been scraped\n",
      "Tamboli’s Pasta & Pizza review page 1 has been scraped\n",
      "Tamboli’s Pasta & Pizza review page 2 has been scraped\n",
      "Tamboli’s Pasta & Pizza has been scraped\n",
      "Char Restaurant https://www.yelp.com/biz/char-restaurant-memphis\n",
      "Char Restaurant review page 0 has been scraped\n",
      "Char Restaurant review page 1 has been scraped\n",
      "Char Restaurant review page 2 has been scraped\n",
      "Char Restaurant has been scraped\n",
      "SALT|SOY https://www.yelp.com/biz/salt-soy-memphis\n",
      "SALT|SOY review page 0 has been scraped\n",
      "SALT|SOY review page 1 has been scraped\n",
      "SALT|SOY review page 2 has been scraped\n",
      "SALT|SOY has been scraped\n",
      "Sunrise Memphis https://www.yelp.com/biz/sunrise-memphis-memphis\n",
      "Sunrise Memphis review page 0 has been scraped\n",
      "Sunrise Memphis review page 1 has been scraped\n",
      "Sunrise Memphis review page 2 has been scraped\n",
      "Sunrise Memphis has been scraped\n",
      "Alchemy https://www.yelp.com/biz/alchemy-memphis-2\n",
      "Alchemy review page 0 has been scraped\n",
      "Alchemy review page 1 has been scraped\n",
      "Alchemy review page 2 has been scraped\n",
      "Alchemy has been scraped\n",
      "Babalu https://www.yelp.com/biz/babalu-memphis\n",
      "Babalu review page 0 has been scraped\n",
      "Babalu review page 1 has been scraped\n",
      "Babalu review page 2 has been scraped\n",
      "Babalu has been scraped\n",
      "Brother Juniper’s https://www.yelp.com/biz/brother-junipers-memphis\n",
      "Brother Juniper’s review page 0 has been scraped\n",
      "Brother Juniper’s review page 1 has been scraped\n",
      "Brother Juniper’s review page 2 has been scraped\n",
      "Brother Juniper’s has been scraped\n",
      "Cafe 1912 https://www.yelp.com/biz/cafe-1912-memphis\n",
      "Cafe 1912 review page 0 has been scraped\n",
      "Cafe 1912 review page 1 has been scraped\n",
      "Cafe 1912 review page 2 has been scraped\n",
      "Cafe 1912 has been scraped\n",
      "Hammer & Ale https://www.yelp.com/biz/hammer-and-ale-memphis\n",
      "Hammer & Ale review page 0 has been scraped\n",
      "Hammer & Ale review page 1 has been scraped\n",
      "Hammer & Ale review page 2 has been scraped\n",
      "Hammer & Ale has been scraped\n",
      "Magnolia & May https://www.yelp.com/biz/magnolia-and-may-memphis\n",
      "Magnolia & May review page 0 has been scraped\n",
      "Magnolia & May review page 1 has been scraped\n",
      "Magnolia & May review page 2 has been scraped\n",
      "Magnolia & May has been scraped\n",
      "Elwood’s Shack https://www.yelp.com/biz/elwoods-shack-memphis\n",
      "Elwood’s Shack review page 0 has been scraped\n",
      "Elwood’s Shack review page 1 has been scraped\n",
      "Elwood’s Shack review page 2 has been scraped\n",
      "Elwood’s Shack has been scraped\n",
      "ECCO on Overton Park https://www.yelp.com/biz/ecco-on-overton-park-memphis\n",
      "ECCO on Overton Park review page 0 has been scraped\n",
      "ECCO on Overton Park review page 1 has been scraped\n",
      "ECCO on Overton Park review page 2 has been scraped\n",
      "ECCO on Overton Park has been scraped\n",
      "Maximo’s On Broad https://www.yelp.com/biz/maximos-on-broad-memphis\n",
      "Maximo’s On Broad review page 0 has been scraped\n",
      "Maximo’s On Broad review page 1 has been scraped\n",
      "Maximo’s On Broad review page 2 has been scraped\n",
      "Maximo’s On Broad has been scraped\n",
      "Restaurant Iris https://www.yelp.com/biz/restaurant-iris-memphis\n",
      "Restaurant Iris review page 0 has been scraped\n",
      "Restaurant Iris review page 1 has been scraped\n",
      "Restaurant Iris review page 2 has been scraped\n",
      "Restaurant Iris has been scraped\n",
      "Belly Acres - Overton Square https://www.yelp.com/biz/belly-acres-overton-square-memphis\n",
      "Belly Acres - Overton Square review page 0 has been scraped\n",
      "Belly Acres - Overton Square review page 1 has been scraped\n",
      "Belly Acres - Overton Square review page 2 has been scraped\n",
      "Belly Acres - Overton Square has been scraped\n",
      "Hattie B’s Hot Chicken - Memphis Midtown https://www.yelp.com/biz/hattie-bs-hot-chicken-memphis-midtown-memphis-3\n",
      "Hattie B’s Hot Chicken - Memphis Midtown review page 0 has been scraped\n",
      "Hattie B’s Hot Chicken - Memphis Midtown review page 1 has been scraped\n",
      "Hattie B’s Hot Chicken - Memphis Midtown review page 2 has been scraped\n",
      "Hattie B’s Hot Chicken - Memphis Midtown has been scraped\n",
      "fam https://www.yelp.com/biz/fam-memphis-2\n",
      "fam review page 0 has been scraped\n",
      "fam review page 1 has been scraped\n",
      "fam review page 2 has been scraped\n",
      "fam has been scraped\n",
      "Hustle & Dough https://www.yelp.com/biz/hustle-and-dough-memphis\n",
      "Hustle & Dough review page 0 has been scraped\n",
      "Hustle & Dough review page 1 has been scraped\n",
      "Hustle & Dough review page 2 has been scraped\n",
      "Hustle & Dough has been scraped\n",
      "The Cove https://www.yelp.com/biz/the-cove-memphis\n",
      "The Cove review page 0 has been scraped\n",
      "The Cove review page 1 has been scraped\n",
      "The Cove review page 2 has been scraped\n",
      "The Cove has been scraped\n",
      "Mahogany Memphis https://www.yelp.com/biz/mahogany-memphis-memphis\n",
      "Mahogany Memphis review page 0 has been scraped\n",
      "Mahogany Memphis review page 1 has been scraped\n",
      "Mahogany Memphis review page 2 has been scraped\n",
      "Mahogany Memphis has been scraped\n",
      "ACRE https://www.yelp.com/biz/acre-memphis-3\n",
      "ACRE review page 0 has been scraped\n",
      "ACRE review page 1 has been scraped\n",
      "ACRE review page 2 has been scraped\n",
      "ACRE has been scraped\n",
      "The World Famous Hernando’s Hide-A-Way Memphis https://www.yelp.com/biz/the-world-famous-hernando-s-hide-a-way-memphis-memphis\n",
      "The World Famous Hernando’s Hide-A-Way Memphis review page 0 has been scraped\n",
      "The World Famous Hernando’s Hide-A-Way Memphis review page 1 has been scraped\n",
      "The World Famous Hernando’s Hide-A-Way Memphis review page 2 has been scraped\n",
      "The World Famous Hernando’s Hide-A-Way Memphis has been scraped\n",
      "Central BBQ https://www.yelp.com/biz/central-bbq-memphis-2\n",
      "Central BBQ review page 0 has been scraped\n",
      "Central BBQ review page 1 has been scraped\n",
      "Central BBQ review page 2 has been scraped\n",
      "Central BBQ has been scraped\n",
      "Soul Fish Cafe https://www.yelp.com/biz/soul-fish-cafe-memphis-3\n",
      "Soul Fish Cafe review page 0 has been scraped\n",
      "Soul Fish Cafe review page 1 has been scraped\n",
      "Soul Fish Cafe review page 2 has been scraped\n",
      "Soul Fish Cafe has been scraped\n",
      "Gus’s World Famous Fried Chicken https://www.yelp.com/biz/guss-world-famous-fried-chicken-memphis-3\n",
      "Gus’s World Famous Fried Chicken review page 0 has been scraped\n",
      "Gus’s World Famous Fried Chicken review page 1 has been scraped\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gus’s World Famous Fried Chicken review page 2 has been scraped\n",
      "Gus’s World Famous Fried Chicken has been scraped\n",
      "The Fry Guy https://www.yelp.com/biz/the-fry-guy-memphis\n",
      "The Fry Guy review page 0 has been scraped\n",
      "The Fry Guy review page 1 has been scraped\n",
      "The Fry Guy review page 2 has been scraped\n",
      "The Fry Guy has been scraped\n",
      "Takashi Bistro https://www.yelp.com/biz/takashi-bistro-memphis\n",
      "Takashi Bistro review page 0 has been scraped\n",
      "Takashi Bistro review page 1 has been scraped\n",
      "Takashi Bistro review page 2 has been scraped\n",
      "Takashi Bistro has been scraped\n",
      "Railgarten https://www.yelp.com/biz/railgarten-memphis-2\n",
      "Error in stage 1\n",
      "Slim and Husky’s Pizza Beeria https://www.yelp.com/biz/slim-and-huskys-pizza-beeria-memphis\n",
      "Error in stage 1\n",
      "Parish Grocery https://www.yelp.com/biz/parish-grocery-memphis-2\n",
      "Error in stage 1\n",
      "The Crazy Noodle https://www.yelp.com/biz/the-crazy-noodle-memphis\n",
      "Error in stage 1\n",
      "DWJ 2 https://www.yelp.com/biz/dwj-2-memphis\n",
      "Error in stage 1\n",
      "Tsunami Restaurant https://www.yelp.com/biz/tsunami-restaurant-memphis\n",
      "Error in stage 1\n",
      "Bari Ristorante https://www.yelp.com/biz/bari-ristorante-memphis-2\n",
      "Error in stage 1\n",
      "Waffle Cream https://www.yelp.com/biz/waffle-cream-memphis\n",
      "Error in stage 1\n",
      "Catherine and Mary’s https://www.yelp.com/biz/catherine-and-marys-memphis-2\n",
      "Error in stage 1\n",
      "117 Prime https://www.yelp.com/biz/117-prime-memphis\n",
      "Error in stage 1\n",
      "Pho Binh https://www.yelp.com/biz/pho-binh-memphis\n",
      "Error in stage 1\n",
      "Boscos Restaurant & Brewing Company https://www.yelp.com/biz/boscos-restaurant-and-brewing-company-memphis\n",
      "Error in stage 1\n",
      "Happy Greek Cafe https://www.yelp.com/biz/happy-greek-cafe-memphis-2\n",
      "Error in stage 1\n",
      "Momma’s https://www.yelp.com/biz/mommas-memphis\n",
      "Error in stage 1\n",
      "Taqueria El Burrito Express https://www.yelp.com/biz/taqueria-el-burrito-express-memphis\n",
      "Error in stage 1\n",
      "Blues City Cafe https://www.yelp.com/biz/blues-city-cafe-memphis\n",
      "Error in stage 1\n",
      "Lafayette’s Music Room https://www.yelp.com/biz/lafayettes-music-room-memphis\n",
      "Error in stage 1\n",
      "Ono Poke https://www.yelp.com/biz/ono-poke-memphis-4\n",
      "Error in stage 1\n",
      "Casablanca https://www.yelp.com/biz/casablanca-memphis-2\n",
      "Error in stage 1\n",
      "Aldo’s Pizza Pies - Midtown https://www.yelp.com/biz/aldos-pizza-pies-midtown-memphis\n",
      "Error in stage 1\n",
      "Hazel’s Lucky Dice Delicatessen https://www.yelp.com/biz/hazel-s-lucky-dice-delicatessen-memphis\n",
      "Error in stage 1\n",
      "Inspire Community Cafe https://www.yelp.com/biz/inspire-community-cafe-memphis-3\n",
      "Error in stage 1\n",
      "SOB East https://www.yelp.com/biz/sob-east-memphis\n",
      "Error in stage 1\n",
      "Joe’s Fried Chicken https://www.yelp.com/biz/joes-fried-chicken-memphis-8\n",
      "Error in stage 1\n",
      "Young Avenue Deli https://www.yelp.com/biz/young-avenue-deli-memphis\n",
      "Error in stage 1\n",
      "Matty And Patty’s https://www.yelp.com/biz/matty-and-patty-s-memphis\n",
      "Error in stage 1\n",
      "Mr. Rusty’s Real Taste of Chicago https://www.yelp.com/biz/mr-rustys-real-taste-of-chicago-memphis\n",
      "Error in stage 1\n",
      "Barksdale Restaurant https://www.yelp.com/biz/barksdale-restaurant-memphis\n",
      "Error in stage 1\n",
      "CITY MARKET - Cooper Young https://www.yelp.com/biz/city-market-cooper-young-memphis-2\n",
      "Error in stage 1\n",
      "R P Tracks https://www.yelp.com/biz/r-p-tracks-memphis\n",
      "Error in stage 1\n",
      "Folks Folly Prime Steakhouse https://www.yelp.com/biz/folks-folly-prime-steakhouse-memphis\n",
      "Error in stage 1\n",
      "Bayou Bar & Grill https://www.yelp.com/biz/bayou-bar-and-grill-memphis\n",
      "Error in stage 1\n",
      "The Majestic Grille https://www.yelp.com/biz/the-majestic-grille-memphis\n",
      "Error in stage 1\n",
      "Central BBQ https://www.yelp.com/biz/central-bbq-memphis-3\n",
      "Error in stage 1\n",
      "South of Beale https://www.yelp.com/biz/south-of-beale-memphis-6\n",
      "Error in stage 1\n",
      "Taco Ngana https://www.yelp.com/biz/taco-ngana-memphis\n",
      "Error in stage 1\n",
      "Another Broken Egg Cafe https://www.yelp.com/biz/another-broken-egg-cafe-memphis\n",
      "Error in stage 1\n",
      "Another Broken Egg Cafe https://www.yelp.com/biz/another-broken-egg-cafe-memphis-2\n",
      "Error in stage 1\n",
      "Fam https://www.yelp.com/biz/fam-memphis\n",
      "Error in stage 1\n",
      "Kwik Chek https://www.yelp.com/biz/kwik-chek-memphis-2\n",
      "Error in stage 1\n",
      "Crab Island https://www.yelp.com/biz/crab-island-memphis\n",
      "Error in stage 1\n",
      "Café Palladio https://www.yelp.com/biz/caf%C3%A9-palladio-memphis-3\n",
      "Error in stage 1\n",
      "Slider Inn https://www.yelp.com/biz/slider-inn-memphis-2\n",
      "Error in stage 1\n",
      "DWJ Korean BBQ https://www.yelp.com/biz/dwj-korean-bbq-memphis\n",
      "Error in stage 1\n",
      "Maciel’s Tortas and Tacos https://www.yelp.com/biz/maciels-tortas-and-tacos-memphis-6\n",
      "Error in stage 1\n",
      "Chef Tam’s Underground Cafe https://www.yelp.com/biz/chef-tams-underground-cafe-memphis\n",
      "Error in stage 1\n",
      "City Silo Table + Pantry https://www.yelp.com/biz/city-silo-table-pantry-memphis-3\n",
      "Error in stage 1\n",
      "Flight Restaurant & Wine Bar https://www.yelp.com/biz/flight-restaurant-and-wine-bar-memphis\n",
      "Error in stage 1\n",
      "Sugar Grits https://www.yelp.com/biz/sugar-grits-memphis\n",
      "Error in stage 1\n",
      "The Choo https://www.yelp.com/biz/the-choo-memphis\n",
      "Error in stage 1\n",
      "Goodie’s https://www.yelp.com/biz/goodies-memphis-2\n",
      "Error in stage 1\n",
      "Los Comales https://www.yelp.com/biz/los-comales-memphis-2\n",
      "Error in stage 1\n",
      "Staks Pancake Kitchen https://www.yelp.com/biz/staks-pancake-kitchen-memphis\n",
      "Error in stage 1\n",
      "Rizzos by Michael Patrick https://www.yelp.com/biz/rizzos-by-michael-patrick-memphis-2\n",
      "Error in stage 1\n",
      "Fries Guys https://www.yelp.com/biz/fries-guys-memphis\n",
      "Error in stage 1\n",
      "Phuong Long https://www.yelp.com/biz/phuong-long-memphis\n",
      "Error in stage 1\n",
      "Evelyn & Olive https://www.yelp.com/biz/evelyn-and-olive-memphis-2\n",
      "Error in stage 1\n",
      "Bryant’s Breakfast https://www.yelp.com/biz/bryants-breakfast-memphis\n",
      "Error in stage 1\n",
      "LBOE https://www.yelp.com/biz/lboe-memphis\n",
      "Error in stage 1\n",
      "Andrew Michael Italian Kitchen https://www.yelp.com/biz/andrew-michael-italian-kitchen-memphis\n",
      "Error in stage 1\n",
      "Burgerim - Memphis https://www.yelp.com/biz/burgerim-memphis-memphis\n",
      "Error in stage 1\n",
      "Flames Mediterranean & grill https://www.yelp.com/biz/flames-mediterranean-and-grill-memphis-4\n",
      "Error in stage 1\n",
      "Side Street Grill https://www.yelp.com/biz/side-street-grill-memphis\n",
      "Error in stage 1\n",
      "Gia’s cafe https://www.yelp.com/biz/gias-cafe-memphis\n",
      "Error in stage 1\n",
      "Ben Yay’s https://www.yelp.com/biz/ben-yays-memphis\n",
      "Error in stage 1\n",
      "The Gray Canary https://www.yelp.com/biz/the-gray-canary-memphis\n",
      "Error in stage 1\n",
      "Payne’s Bar-B-Que https://www.yelp.com/biz/paynes-bar-b-que-memphis\n",
      "Error in stage 1\n",
      "Stone Soup Cafe & Market https://www.yelp.com/biz/stone-soup-cafe-and-market-memphis-2\n",
      "Error in stage 1\n",
      "Lucchesi’s Beer Garden https://www.yelp.com/biz/lucchesis-beer-garden-memphis\n",
      "Error in stage 1\n",
      "Rendezvous Restaurant https://www.yelp.com/biz/rendezvous-restaurant-memphis-3\n",
      "Error in stage 1\n",
      "Kami Ramen Bar https://www.yelp.com/biz/kami-ramen-bar-memphis\n",
      "Error in stage 1\n",
      "The Bluff https://www.yelp.com/biz/the-bluff-memphis\n",
      "Error in stage 1\n",
      "Babalu https://www.yelp.com/biz/babalu-memphis-2\n",
      "Error in stage 1\n",
      "Bishop https://www.yelp.com/biz/bishop-memphis\n",
      "Error in stage 1\n",
      "Rhodes Dining https://www.yelp.com/biz/rhodes-dining-memphis\n",
      "Error in stage 1\n",
      "Imagine Vegan Cafe https://www.yelp.com/biz/imagine-vegan-cafe-memphis\n",
      "Error in stage 1\n",
      "Sage Restaurant https://www.yelp.com/biz/sage-restaurant-memphis\n",
      "Error in stage 1\n",
      "Belltower Coffeehouse & Studio https://www.yelp.com/biz/belltower-coffeehouse-and-studio-memphis\n",
      "Error in stage 1\n",
      "Houston’s Restaurant https://www.yelp.com/biz/houstons-restaurant-memphis\n",
      "Error in stage 1\n",
      "Local On The Square https://www.yelp.com/biz/local-on-the-square-memphis\n",
      "Error in stage 1\n",
      "Highland Super Submarine Sandwich Shop https://www.yelp.com/biz/highland-super-submarine-sandwich-shop-memphis\n",
      "Error in stage 1\n",
      "Global Café https://www.yelp.com/biz/global-caf%C3%A9-memphis\n",
      "Error in stage 1\n",
      "The Capital Grille https://www.yelp.com/biz/the-capital-grille-memphis-2\n",
      "Error in stage 1\n",
      "Lost Pizza Company - Memphis https://www.yelp.com/biz/lost-pizza-company-memphis-memphis\n",
      "Error in stage 1\n",
      "Andalusia https://www.yelp.com/biz/andalusia-memphis\n",
      "Error in stage 1\n",
      "Otherlands https://www.yelp.com/biz/otherlands-memphis-2\n",
      "Error in stage 1\n",
      "Café Eclectic https://www.yelp.com/biz/caf%C3%A9-eclectic-memphis-7\n",
      "Error in stage 1\n",
      "Cafe Society https://www.yelp.com/biz/cafe-society-memphis\n",
      "Error in stage 1\n",
      "Stick ‘Em https://www.yelp.com/biz/stick-em-memphis\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in stage 1\n",
      "Pizzeria Trasimeno https://www.yelp.com/biz/pizzeria-trasimeno-memphis\n",
      "Error in stage 1\n",
      "Itta Bena https://www.yelp.com/biz/itta-bena-memphis\n",
      "Error in stage 1\n",
      "Tokyo Grill https://www.yelp.com/biz/tokyo-grill-memphis-10\n",
      "Error in stage 1\n",
      "Sabor Caribe https://www.yelp.com/biz/sabor-caribe-memphis\n",
      "Error in stage 1\n",
      "Cook Out https://www.yelp.com/biz/cook-out-memphis\n",
      "Error in stage 1\n",
      "Kim Chai Restaurant https://www.yelp.com/biz/kim-chai-restaurant-memphis\n",
      "Error in stage 1\n",
      "High Point Pizza https://www.yelp.com/biz/high-point-pizza-memphis\n",
      "Error in stage 1\n",
      "Slutty Salsa https://www.yelp.com/biz/slutty-salsa-memphis\n",
      "Error in stage 1\n",
      "Robata Ramen and Yakitori Bar https://www.yelp.com/biz/robata-ramen-and-yakitori-bar-memphis\n",
      "Error in stage 1\n",
      "Arepas Deliciosas https://www.yelp.com/biz/arepas-deliciosas-memphis\n",
      "Error in stage 1\n",
      "Erling Jensen The Restaurant https://www.yelp.com/biz/erling-jensen-the-restaurant-memphis\n",
      "Error in stage 1\n",
      "The Lookout At The Pyramid https://www.yelp.com/biz/the-lookout-at-the-pyramid-memphis\n",
      "Error in stage 1\n",
      "Chez Philippe https://www.yelp.com/biz/chez-philippe-memphis\n",
      "Error in stage 1\n",
      "Flying Fish https://www.yelp.com/biz/flying-fish-memphis-2\n",
      "Error in stage 1\n",
      "Pete & Sam’s Restaurant https://www.yelp.com/biz/pete-and-sams-restaurant-memphis\n",
      "Error in stage 1\n",
      "Orange Mound Grill https://www.yelp.com/biz/orange-mound-grill-memphis\n",
      "Error in stage 1\n",
      "Huey’s - Midtown https://www.yelp.com/biz/hueys-midtown-memphis-4\n",
      "Error in stage 1\n",
      "Dino’s Grill https://www.yelp.com/biz/dinos-grill-memphis\n",
      "Error in stage 1\n",
      "Ciao Bella Italian Grill and Bar https://www.yelp.com/biz/ciao-bella-italian-grill-and-bar-memphis\n",
      "Error in stage 1\n",
      "Freddy’s Frozen Custard & Steakburgers https://www.yelp.com/biz/freddys-frozen-custard-and-steakburgers-memphis-5\n",
      "Error in stage 1\n",
      "Bala’s Bistro https://www.yelp.com/biz/balas-bistro-memphis-2\n",
      "Error in stage 1\n",
      "Venice Kitchen https://www.yelp.com/biz/venice-kitchen-memphis-2\n",
      "Error in stage 1\n",
      "Marciano https://www.yelp.com/biz/marciano-memphis\n",
      "Error in stage 1\n",
      "Dixie Queen https://www.yelp.com/biz/dixie-queen-memphis-17\n",
      "Error in stage 1\n",
      "La Baguette French Bread and Pastry Shop https://www.yelp.com/biz/la-baguette-french-bread-and-pastry-shop-memphis\n",
      "Error in stage 1\n",
      "Gus’s World Famous Fried Chicken https://www.yelp.com/biz/guss-world-famous-fried-chicken-memphis-2\n",
      "Error in stage 1\n",
      "Island Paradise Takeout https://www.yelp.com/biz/island-paradise-takeout-memphis\n",
      "Error in stage 1\n",
      "Ball Hoggerz BBQ https://www.yelp.com/biz/ball-hoggerz-bbq-memphis\n",
      "Error in stage 1\n",
      "The Bar-B-Q Shop https://www.yelp.com/biz/the-bar-b-q-shop-memphis-6\n",
      "Error in stage 1\n",
      "Fino’s from the Hill https://www.yelp.com/biz/finos-from-the-hill-memphis-2\n",
      "Error in stage 1\n",
      "Napa Cafe https://www.yelp.com/biz/napa-cafe-memphis\n",
      "Error in stage 1\n",
      "McEwen’s Memphis - Temp. CLOSED https://www.yelp.com/biz/mcewens-memphis-memphis\n",
      "Error in stage 1\n",
      "Tropical Smoothie Cafe - Opening Soon https://www.yelp.com/biz/tropical-smoothie-cafe-opening-soon-memphis-2\n",
      "Error in stage 1\n",
      "DeeO’s Seafood https://www.yelp.com/biz/deeos-seafood-memphis-2\n",
      "Error in stage 1\n",
      "Pho Saigon https://www.yelp.com/biz/pho-saigon-memphis\n",
      "Error in stage 1\n",
      "Edge Alley https://www.yelp.com/biz/edge-alley-memphis-2\n",
      "Error in stage 1\n",
      "The Vault https://www.yelp.com/biz/the-vault-memphis-6\n",
      "Error in stage 1\n",
      "Cafe Olé https://www.yelp.com/biz/cafe-ol%C3%A9-memphis-3\n",
      "Error in stage 1\n",
      "Mortimer’s Restaurant https://www.yelp.com/biz/mortimers-restaurant-memphis\n",
      "Error in stage 1\n",
      "Ubee’s https://www.yelp.com/biz/ubees-memphis\n",
      "Error in stage 1\n",
      "Suga Shack https://www.yelp.com/biz/suga-shack-memphis\n",
      "Error in stage 1\n",
      "Bardog Tavern https://www.yelp.com/biz/bardog-tavern-memphis\n",
      "Error in stage 1\n",
      "Abyssinia Ethiopian Restaurant https://www.yelp.com/biz/abyssinia-ethiopian-restaurant-memphis-3\n",
      "Error in stage 1\n",
      "Buckley’s Restaurant East https://www.yelp.com/biz/buckleys-restaurant-east-memphis\n",
      "Error in stage 1\n",
      "Bluff City Coffee https://www.yelp.com/biz/bluff-city-coffee-memphis-2\n",
      "Error in stage 1\n",
      "Elena’s Taco Shop https://www.yelp.com/biz/elenas-taco-shop-memphis\n",
      "Error in stage 1\n",
      "The Backlot Sandwich Shop https://www.yelp.com/biz/the-backlot-sandwich-shop-memphis\n",
      "Error in stage 1\n",
      "Cheffie’s Cafe https://www.yelp.com/biz/cheffies-cafe-memphis\n",
      "Error in stage 1\n",
      "Pontotoc https://www.yelp.com/biz/pontotoc-memphis\n",
      "Error in stage 1\n",
      "Mulan Asian Bistro https://www.yelp.com/biz/mulan-asian-bistro-memphis\n",
      "Error in stage 1\n",
      "Garibaldi’s Pizza https://www.yelp.com/biz/garibaldis-pizza-memphis-2\n",
      "Error in stage 1\n",
      "Caritas Village https://www.yelp.com/biz/caritas-village-memphis\n",
      "Error in stage 1\n",
      "River Oaks Restaurant https://www.yelp.com/biz/river-oaks-restaurant-memphis\n",
      "Error in stage 1\n",
      "Coletta’s Italian Restaurant https://www.yelp.com/biz/colettas-italian-restaurant-memphis\n",
      "Error in stage 1\n",
      "Carolina Watershed Memphis https://www.yelp.com/biz/carolina-watershed-memphis-memphis\n",
      "Error in stage 1\n",
      "Celtic Crossing Irish Pub & Restaurant https://www.yelp.com/biz/celtic-crossing-irish-pub-and-restaurant-memphis\n",
      "Error in stage 1\n",
      "Ruth’s Chris Steak House https://www.yelp.com/biz/ruths-chris-steak-house-memphis\n",
      "Error in stage 1\n",
      "Paulette’s Restaurant https://www.yelp.com/biz/paulettes-restaurant-memphis\n",
      "Error in stage 1\n",
      "Agavos Cocina & Tequila https://www.yelp.com/biz/agavos-cocina-and-tequila-memphis\n",
      "Error in stage 1\n",
      "Southall Cafe https://www.yelp.com/biz/southall-cafe-memphis-3\n",
      "Error in stage 1\n",
      "South Memphis Grocery https://www.yelp.com/biz/south-memphis-grocery-memphis\n",
      "Error in stage 1\n",
      "Loflin Yard https://www.yelp.com/biz/loflin-yard-memphis\n",
      "Error in stage 1\n",
      "Cocina Mexicana https://www.yelp.com/biz/cocina-mexicana-memphis\n",
      "Error in stage 1\n",
      "Raw Girls https://www.yelp.com/biz/raw-girls-memphis-3\n",
      "Error in stage 1\n",
      "Texas de Brazil https://www.yelp.com/biz/texas-de-brazil-memphis-3\n",
      "Error in stage 1\n",
      "Picosos https://www.yelp.com/biz/picosos-memphis\n",
      "Error in stage 1\n",
      "The Woman’s Exchange of Memphis - Tea Room https://www.yelp.com/biz/the-womans-exchange-of-memphis-tea-room-memphis\n",
      "Error in stage 1\n",
      "Fleming’s Prime Steakhouse & Wine Bar https://www.yelp.com/biz/fleming-s-prime-steakhouse-and-wine-bar-memphis\n",
      "Error in stage 1\n",
      "Willie Mae’s Southern Soul Cooking https://www.yelp.com/biz/willie-maes-southern-soul-cooking-memphis-2\n",
      "Error in stage 1\n",
      "Marlowe’s Ribs & Restaurant https://www.yelp.com/biz/marlowes-ribs-and-restaurant-memphis-2\n",
      "Error in stage 1\n",
      "Seasons 52 https://www.yelp.com/biz/seasons-52-memphis-3\n",
      "Error in stage 1\n",
      "Redhook Cajun Seafood & Bar https://www.yelp.com/biz/redhook-cajun-seafood-and-bar-memphis\n",
      "Error in stage 1\n",
      "Mesquite Chop House https://www.yelp.com/biz/mesquite-chop-house-memphis\n",
      "Error in stage 1\n",
      "Queen Of Sheba https://www.yelp.com/biz/queen-of-sheba-memphis\n",
      "Error in stage 1\n",
      "Peggy’s Heavenly Home Cooking https://www.yelp.com/biz/peggys-heavenly-home-cooking-memphis-3\n",
      "Error in stage 1\n"
     ]
    }
   ],
   "source": [
    "for row in df.iterrows():\n",
    "    try:\n",
    "        #### Get the business name and url ####\n",
    "        business_name = row[1]['name']\n",
    "        business_url = row[1]['url']\n",
    "        print(business_name, business_url)\n",
    "\n",
    "        #### Request the page ####\n",
    "        html = requests.get(business_url)\n",
    "        soup = BeautifulSoup(html.content, 'html.parser')\n",
    "        \n",
    "        #### Average Rating ####\n",
    "        try:\n",
    "            mean_rating = soup.select('.i-stars--large-5__373c0__1GcGD')\n",
    "            mean_rating = float(mean_rating[0].attrs['aria-label'].split(' ')[0])\n",
    "        except:\n",
    "            try:\n",
    "                mean_rating = soup.select('.i-stars--large-4-half__373c0__2lYkD')\n",
    "                mean_rating = float(mean_rating[0].attrs['aria-label'].split(' ')[0])\n",
    "            except:\n",
    "                try:\n",
    "                    mean_rating = soup.select('.i-stars--large-4__373c0__1d6HV')\n",
    "                    mean_rating = float(mean_rating[0].attrs['aria-label'].split(' ')[0])\n",
    "                except:\n",
    "                    try:\n",
    "                        mean_rating = soup.select('.i-stars--large-3-half__373c0__2z4jR')\n",
    "                        mean_rating = float(mean_rating[0].attrs['aria-label'].split(' ')[0])\n",
    "                    except:\n",
    "                        try: \n",
    "                            mean_rating = soup.select('.i-stars--large-3__373c0__3_Jon')\n",
    "                            mean_rating = float(mean_rating[0].attrs['aria-label'].split(' ')[0])\n",
    "                        except:\n",
    "                            mean_rating = 0\n",
    "\n",
    "\n",
    "        # Total Number of Reviews\n",
    "        total_reviews = soup.select('.arrange-unit-fill__373c0__17z0h.nowrap__373c0__1_N1j .css-bq71j2')\n",
    "        total_reviews = int(total_reviews[0].string.split(' ')[0])\n",
    "\n",
    "        # Claimed Status\n",
    "        try:\n",
    "            claimed_status = soup.select('.css-1xxismk .css-166la90')[0].string\n",
    "            claimed_status = 0\n",
    "        except:\n",
    "            claimed_status = 1\n",
    "\n",
    "        # Price Category\n",
    "        try:\n",
    "            price_cat = soup.select('.margin-r1__373c0__zyKmV+ .border-color--default__373c0__2oFDT .css-1xxismk')\n",
    "            price_cat = len((price_cat[0].text).strip())\n",
    "        except:\n",
    "            price_cat = None\n",
    "            \n",
    "        # Restaurant Categories\n",
    "        categories = soup.select('.css-bq71j2 .css-166la90')\n",
    "        categories = [x.string for x in categories]\n",
    "\n",
    "        # Total number of Photos\n",
    "        try:\n",
    "            tot_n_photos = soup.select('.css-1fepc68 .css-ardur')\n",
    "            tot_n_photos = int(tot_n_photos[0].string.split(' ')[1])\n",
    "        except:\n",
    "            tot_n_photos = 0\n",
    "            \n",
    "        # Health Score\n",
    "        try:\n",
    "            health_score = soup.select('.label-spacing-v2__373c0__PBYkt')\n",
    "            health_score = int(health_score[0].string.split(' ')[0])\n",
    "        except:\n",
    "            health_score = None\n",
    "            \n",
    "        # Covid Updates\n",
    "        cov_updates = soup.select('.margin-b1__373c0__1khoT .css-1h1j0y3')\n",
    "        cov_updates = [x.string for x in cov_updates]\n",
    "\n",
    "        # Street Address\n",
    "        st_address = soup.select('.css-1bmgof7 .raw__373c0__3rcx7')\n",
    "        st_address = st_address[0].string    \n",
    "\n",
    "        business_info = [business_name, business_url, total_reviews, claimed_status,\n",
    "                             price_cat, categories, tot_n_photos, health_score, cov_updates, st_address]\n",
    "\n",
    "        #### Get the data for multiple review pages ####\n",
    "        #### Set the number of review pages that should be scraped ####\n",
    "\n",
    "        for i in range(3):\n",
    "            try:\n",
    "                url2 = business_url + '?start=' + str(i * 10)\n",
    "                html = requests.get(url2)\n",
    "                soup = BeautifulSoup(html.content, 'html.parser')\n",
    "\n",
    "                #### Names\n",
    "                names = soup.select('.css-m6anxm .css-166la90')\n",
    "                review_names = []\n",
    "\n",
    "                for name in names:\n",
    "                    review_names.append(name.string)\n",
    "\n",
    "                #### Ratings\n",
    "                ratings = soup.select('.margin-b1-5__373c0__2Wblx .overflow--hidden__373c0__2B0kz')\n",
    "\n",
    "                review_ratings = []\n",
    "\n",
    "                for rating in ratings:\n",
    "                    review_ratings.append(float(re.sub(' star rating', \n",
    "                                                       '',rating.attrs['aria-label'])))\n",
    "                #### Dates\n",
    "                dates = soup.select('.margin-b1-5__373c0__2Wblx .css-e81eai')\n",
    "\n",
    "                review_dates = []\n",
    "\n",
    "                for date in dates:\n",
    "                    review_dates.append(date.string)\n",
    "\n",
    "                #### Text\n",
    "                texts = soup.select('.css-n6i4z7 .raw__373c0__3rcx7')\n",
    "                review_texts = []\n",
    "\n",
    "                for text in texts:\n",
    "                    review_texts.append(text.get_text())\n",
    "\n",
    "                #### Reviewer's Location\n",
    "                reviewer_loc = soup.select('.arrange-unit-fill__373c0__17z0h .border-color--default__373c0__2oFDT .border-color--default__373c0__2oFDT .border-color--default__373c0__2oFDT .css-n6i4z7')\n",
    "\n",
    "                reviewer_city = []\n",
    "                reviewer_state = []\n",
    "\n",
    "                for loc in reviewer_loc:\n",
    "                    if len(loc.get_text().split(',')) == 2:\n",
    "                        reviewer_city.append(loc.get_text().split(',')[0])\n",
    "                        reviewer_state.append(loc.get_text().split(',')[1])\n",
    "                    elif len(loc.get_text().split(',')) == 3:\n",
    "                        reviewer_city.append(loc.get_text().split(',')[1])\n",
    "                        reviewer_state.append(loc.get_text().split(',')[2])\n",
    "                    else:\n",
    "                        reviewer_city.append(None)\n",
    "                        reviewer_city.append(None)    \n",
    "\n",
    "                #### Votes\n",
    "                votes = soup.select('.arrange-unit-fill__373c0__17z0h .display--inline__373c0__1DbOG .css-1ha1j8d')\n",
    "\n",
    "                useful_votes = []\n",
    "                funny_votes = []\n",
    "                cool_votes = []\n",
    "\n",
    "                for vote in votes:\n",
    "                    if vote.get_text().split(' ')[0] == 'Useful':\n",
    "                        try:\n",
    "                            useful_votes.append(vote.get_text().split(' ')[1])\n",
    "                        except:\n",
    "                            useful_votes.append(0)\n",
    "                    elif vote.get_text().split(' ')[0] == 'Funny':\n",
    "                        try:\n",
    "                            funny_votes.append(vote.get_text().split(' ')[1])\n",
    "                        except:\n",
    "                            funny_votes.append(0)\n",
    "                    else:\n",
    "                        try:\n",
    "                            cool_votes.append(vote.get_text().split(' ')[1])\n",
    "                        except:\n",
    "                            cool_votes.append(0)    \n",
    "                review_vars1 = [review_names, review_ratings, review_dates, \n",
    "                                review_texts, reviewer_city, reviewer_state,\n",
    "                                useful_votes, funny_votes, cool_votes]\n",
    "\n",
    "\n",
    "                #### Getting the inconsistent variables ####\n",
    "                n_photos = []\n",
    "                r_friends = []\n",
    "                r_reviews = []\n",
    "                r_photos = []\n",
    "                elite_tag = []\n",
    "\n",
    "                raw_reviews = soup.select('.review__373c0__13kpL')\n",
    "                for raw_review in raw_reviews:\n",
    "\n",
    "                    ### Number of Photos in the review ###\n",
    "                    raw_n_photos = raw_review.select('.margin-b0-5__373c0__Fo75u:nth-child(1) .css-166la90')\n",
    "                    if len(raw_n_photos) == 0:\n",
    "                        n_photos.append(0)\n",
    "                    else:\n",
    "                        try:\n",
    "                            n_photos.append(int(raw_n_photos[0].string.split()[0]))\n",
    "                        except:\n",
    "                            n_photos.append(0)\n",
    "\n",
    "                    ### Profile tags (# Friends, # Reviews, # Pictures) ###   \n",
    "                    raw_tags = raw_review.select('.arrange-unit-fill__373c0__17z0h .border-color--default__373c0__2oFDT .border-color--default__373c0__2oFDT .border-color--default__373c0__2oFDT .border-color--default__373c0__2oFDT .border-color--default__373c0__2oFDT .border-color--default__373c0__2oFDT .css-1dgkz3l')\n",
    "                    r_friends.append(int(raw_tags[0].string))\n",
    "                    r_reviews.append(int(raw_tags[1].string))\n",
    "                    try:\n",
    "                        r_photos.append(int(raw_tags[2].string))\n",
    "                    except:\n",
    "                        r_photos.append(0)\n",
    "\n",
    "                    ### The \"Yelp Elite Squad\" tag ###\n",
    "                    raw_elite_tag = raw_review.select('.css-1hx6l2b')\n",
    "                    if len(raw_elite_tag) == 0:\n",
    "                        elite_tag.append(0)\n",
    "                    else:\n",
    "                        elite_tag.append(1)\n",
    "\n",
    "                #### Combining all variables into 1 dataset ####\n",
    "\n",
    "                business_names = [business_name] * len(review_names)\n",
    "                business_urls = [business_url] * len(review_names)\n",
    "                mean_ratings = [mean_rating] * len(review_names)\n",
    "                total_reviews_all = [total_reviews] * len(review_names)\n",
    "                claimed_status_all = [claimed_status] * len(review_names)\n",
    "                price_cat_all = [price_cat] * len(review_names)\n",
    "                categories_all = [categories] * len(review_names)\n",
    "                tot_n_photos_all = [tot_n_photos] * len(review_names)\n",
    "                health_score_all = [health_score] * len(review_names)\n",
    "                cov_updates_all = [cov_updates] * len(review_names)\n",
    "                st_address_all = [st_address] * len(review_names)\n",
    "\n",
    "                review_data = list(zip(business_names, business_urls, mean_ratings, total_reviews_all, \n",
    "                                       claimed_status_all,price_cat_all, categories_all, \n",
    "                                       tot_n_photos_all, health_score_all, cov_updates_all,\n",
    "                                       st_address_all, review_names, review_ratings, review_dates, \n",
    "                                       review_texts, n_photos, r_friends, r_reviews, \n",
    "                                       r_photos, elite_tag))\n",
    "\n",
    "                df_reviews = pd.DataFrame(data=review_data,\n",
    "                                          columns = ['name_business', 'url', 'mean_rating', 'total_reviews', \n",
    "                                                     'claimed_status','price_cat', 'categories', \n",
    "                                                     'tot_n_photos', 'health_score', 'cov_updates', \n",
    "                                                     'st_address','username', 'rating', \n",
    "                                                     'date_review', 'text', 'n_photos',\n",
    "                                                     'r_friends', 'r_reviews', 'r_photos',\n",
    "                                                     'elite_tag'])\n",
    "\n",
    "                with open('memphis_reviews.csv', 'a', newline='', encoding='utf-8') as f:\n",
    "                    df_reviews.to_csv(f, index=False, header=False)\n",
    "\n",
    "                print('{} review page {} has been scraped'.format(row[1]['name'], str(i)))\n",
    "                time.sleep(2)\n",
    "                \n",
    "            except:\n",
    "                print('Error in stage 2')\n",
    "\n",
    "        print('{} has been scraped'.format(row[1]['name']))    \n",
    "        time.sleep(2)\n",
    "    \n",
    "    except:\n",
    "        print('Error in stage 1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After getting blocked on Yelp, we can continue the scraper where it left off by looking at the last business in the csv, and continuing from the next business in the master list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews = pd.read_csv('memphis_reviews.csv', header=None, encoding='utf-8')\n",
    "idx = df.index[df['name'] == df_reviews.iloc[-1][0]]\n",
    "df_continue = df[(idx[0]+1):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Picosos https://www.yelp.com/biz/picosos-memphis\n",
      "Picosos review page 0 has been scraped\n",
      "Picosos review page 1 has been scraped\n",
      "Picosos review page 2 has been scraped\n",
      "Picosos has been scraped\n",
      "The Woman’s Exchange of Memphis - Tea Room https://www.yelp.com/biz/the-womans-exchange-of-memphis-tea-room-memphis\n",
      "The Woman’s Exchange of Memphis - Tea Room review page 0 has been scraped\n",
      "The Woman’s Exchange of Memphis - Tea Room review page 1 has been scraped\n",
      "The Woman’s Exchange of Memphis - Tea Room review page 2 has been scraped\n",
      "The Woman’s Exchange of Memphis - Tea Room has been scraped\n",
      "Fleming’s Prime Steakhouse & Wine Bar https://www.yelp.com/biz/fleming-s-prime-steakhouse-and-wine-bar-memphis\n",
      "Fleming’s Prime Steakhouse & Wine Bar review page 0 has been scraped\n",
      "Fleming’s Prime Steakhouse & Wine Bar review page 1 has been scraped\n",
      "Fleming’s Prime Steakhouse & Wine Bar review page 2 has been scraped\n",
      "Fleming’s Prime Steakhouse & Wine Bar has been scraped\n",
      "Willie Mae’s Southern Soul Cooking https://www.yelp.com/biz/willie-maes-southern-soul-cooking-memphis-2\n",
      "Willie Mae’s Southern Soul Cooking review page 0 has been scraped\n",
      "Willie Mae’s Southern Soul Cooking review page 1 has been scraped\n",
      "Willie Mae’s Southern Soul Cooking review page 2 has been scraped\n",
      "Willie Mae’s Southern Soul Cooking has been scraped\n",
      "Marlowe’s Ribs & Restaurant https://www.yelp.com/biz/marlowes-ribs-and-restaurant-memphis-2\n",
      "Marlowe’s Ribs & Restaurant review page 0 has been scraped\n",
      "Marlowe’s Ribs & Restaurant review page 1 has been scraped\n",
      "Marlowe’s Ribs & Restaurant review page 2 has been scraped\n",
      "Marlowe’s Ribs & Restaurant has been scraped\n",
      "Seasons 52 https://www.yelp.com/biz/seasons-52-memphis-3\n",
      "Seasons 52 review page 0 has been scraped\n",
      "Seasons 52 review page 1 has been scraped\n",
      "Seasons 52 review page 2 has been scraped\n",
      "Seasons 52 has been scraped\n",
      "Redhook Cajun Seafood & Bar https://www.yelp.com/biz/redhook-cajun-seafood-and-bar-memphis\n",
      "Redhook Cajun Seafood & Bar review page 0 has been scraped\n",
      "Redhook Cajun Seafood & Bar review page 1 has been scraped\n",
      "Redhook Cajun Seafood & Bar review page 2 has been scraped\n",
      "Redhook Cajun Seafood & Bar has been scraped\n",
      "Mesquite Chop House https://www.yelp.com/biz/mesquite-chop-house-memphis\n",
      "Mesquite Chop House review page 0 has been scraped\n",
      "Mesquite Chop House review page 1 has been scraped\n",
      "Mesquite Chop House review page 2 has been scraped\n",
      "Mesquite Chop House has been scraped\n",
      "Queen Of Sheba https://www.yelp.com/biz/queen-of-sheba-memphis\n",
      "Queen Of Sheba review page 0 has been scraped\n",
      "Queen Of Sheba review page 1 has been scraped\n",
      "Queen Of Sheba review page 2 has been scraped\n",
      "Queen Of Sheba has been scraped\n",
      "Peggy’s Heavenly Home Cooking https://www.yelp.com/biz/peggys-heavenly-home-cooking-memphis-3\n",
      "Peggy’s Heavenly Home Cooking review page 0 has been scraped\n",
      "Peggy’s Heavenly Home Cooking review page 1 has been scraped\n",
      "Peggy’s Heavenly Home Cooking review page 2 has been scraped\n",
      "Peggy’s Heavenly Home Cooking has been scraped\n"
     ]
    }
   ],
   "source": [
    "for row in df_continue.iterrows():\n",
    "    try:\n",
    "        #### Get the business name and url ####\n",
    "        business_name = row[1]['name']\n",
    "        business_url = row[1]['url']\n",
    "        print(business_name, business_url)\n",
    "\n",
    "        #### Request the page ####\n",
    "        html = requests.get(business_url)\n",
    "        soup = BeautifulSoup(html.content, 'html.parser')\n",
    "        \n",
    "        #### Average Rating ####\n",
    "        try:\n",
    "            mean_rating = soup.select('.i-stars--large-5__373c0__1GcGD')\n",
    "            mean_rating = float(mean_rating[0].attrs['aria-label'].split(' ')[0])\n",
    "        except:\n",
    "            try:\n",
    "                mean_rating = soup.select('.i-stars--large-4-half__373c0__2lYkD')\n",
    "                mean_rating = float(mean_rating[0].attrs['aria-label'].split(' ')[0])\n",
    "            except:\n",
    "                try:\n",
    "                    mean_rating = soup.select('.i-stars--large-4__373c0__1d6HV')\n",
    "                    mean_rating = float(mean_rating[0].attrs['aria-label'].split(' ')[0])\n",
    "                except:\n",
    "                    try:\n",
    "                        mean_rating = soup.select('.i-stars--large-3-half__373c0__2z4jR')\n",
    "                        mean_rating = float(mean_rating[0].attrs['aria-label'].split(' ')[0])\n",
    "                    except:\n",
    "                        try: \n",
    "                            mean_rating = soup.select('.i-stars--large-3__373c0__3_Jon')\n",
    "                            mean_rating = float(mean_rating[0].attrs['aria-label'].split(' ')[0])\n",
    "                        except:\n",
    "                            mean_rating = 0\n",
    "\n",
    "\n",
    "        # Total Number of Reviews\n",
    "        total_reviews = soup.select('.arrange-unit-fill__373c0__17z0h.nowrap__373c0__1_N1j .css-bq71j2')\n",
    "        total_reviews = int(total_reviews[0].string.split(' ')[0])\n",
    "\n",
    "        # Claimed Status\n",
    "        try:\n",
    "            claimed_status = soup.select('.css-1xxismk .css-166la90')[0].string\n",
    "            claimed_status = 0\n",
    "        except:\n",
    "            claimed_status = 1\n",
    "\n",
    "        # Price Category\n",
    "        try:\n",
    "            price_cat = soup.select('.margin-r1__373c0__zyKmV+ .border-color--default__373c0__2oFDT .css-1xxismk')\n",
    "            price_cat = len((price_cat[0].text).strip())\n",
    "        except:\n",
    "            price_cat = None\n",
    "            \n",
    "        # Restaurant Categories\n",
    "        categories = soup.select('.css-bq71j2 .css-166la90')\n",
    "        categories = [x.string for x in categories]\n",
    "\n",
    "        # Total number of Photos\n",
    "        try:\n",
    "            tot_n_photos = soup.select('.css-1fepc68 .css-ardur')\n",
    "            tot_n_photos = int(tot_n_photos[0].string.split(' ')[1])\n",
    "        except:\n",
    "            tot_n_photos = 0\n",
    "            \n",
    "        # Health Score\n",
    "        try:\n",
    "            health_score = soup.select('.label-spacing-v2__373c0__PBYkt')\n",
    "            health_score = int(health_score[0].string.split(' ')[0])\n",
    "        except:\n",
    "            health_score = None\n",
    "            \n",
    "        # Covid Updates\n",
    "        cov_updates = soup.select('.margin-b1__373c0__1khoT .css-1h1j0y3')\n",
    "        cov_updates = [x.string for x in cov_updates]\n",
    "\n",
    "        # Street Address\n",
    "        st_address = soup.select('.css-1bmgof7 .raw__373c0__3rcx7')\n",
    "        st_address = st_address[0].string    \n",
    "\n",
    "        business_info = [business_name, business_url, total_reviews, claimed_status,\n",
    "                             price_cat, categories, tot_n_photos, health_score, cov_updates, st_address]\n",
    "\n",
    "        #### Get the data for multiple review pages ####\n",
    "        #### Set the number of review pages that should be scraped ####\n",
    "\n",
    "        for i in range(3):\n",
    "            try:\n",
    "                url2 = business_url + '?start=' + str(i * 10)\n",
    "                html = requests.get(url2)\n",
    "                soup = BeautifulSoup(html.content, 'html.parser')\n",
    "\n",
    "                #### Names\n",
    "                names = soup.select('.css-m6anxm .css-166la90')\n",
    "                review_names = []\n",
    "\n",
    "                for name in names:\n",
    "                    review_names.append(name.string)\n",
    "\n",
    "                #### Ratings\n",
    "                ratings = soup.select('.margin-b1-5__373c0__2Wblx .overflow--hidden__373c0__2B0kz')\n",
    "\n",
    "                review_ratings = []\n",
    "\n",
    "                for rating in ratings:\n",
    "                    review_ratings.append(float(re.sub(' star rating', \n",
    "                                                       '',rating.attrs['aria-label'])))\n",
    "                #### Dates\n",
    "                dates = soup.select('.margin-b1-5__373c0__2Wblx .css-e81eai')\n",
    "\n",
    "                review_dates = []\n",
    "\n",
    "                for date in dates:\n",
    "                    review_dates.append(date.string)\n",
    "\n",
    "                #### Text\n",
    "                texts = soup.select('.css-n6i4z7 .raw__373c0__3rcx7')\n",
    "                review_texts = []\n",
    "\n",
    "                for text in texts:\n",
    "                    review_texts.append(text.get_text())\n",
    "\n",
    "                #### Reviewer's Location\n",
    "                reviewer_loc = soup.select('.arrange-unit-fill__373c0__17z0h .border-color--default__373c0__2oFDT .border-color--default__373c0__2oFDT .border-color--default__373c0__2oFDT .css-n6i4z7')\n",
    "\n",
    "                reviewer_city = []\n",
    "                reviewer_state = []\n",
    "\n",
    "                for loc in reviewer_loc:\n",
    "                    if len(loc.get_text().split(',')) == 2:\n",
    "                        reviewer_city.append(loc.get_text().split(',')[0])\n",
    "                        reviewer_state.append(loc.get_text().split(',')[1])\n",
    "                    elif len(loc.get_text().split(',')) == 3:\n",
    "                        reviewer_city.append(loc.get_text().split(',')[1])\n",
    "                        reviewer_state.append(loc.get_text().split(',')[2])\n",
    "                    else:\n",
    "                        reviewer_city.append(None)\n",
    "                        reviewer_city.append(None)    \n",
    "\n",
    "                #### Votes\n",
    "                votes = soup.select('.arrange-unit-fill__373c0__17z0h .display--inline__373c0__1DbOG .css-1ha1j8d')\n",
    "\n",
    "                useful_votes = []\n",
    "                funny_votes = []\n",
    "                cool_votes = []\n",
    "\n",
    "                for vote in votes:\n",
    "                    if vote.get_text().split(' ')[0] == 'Useful':\n",
    "                        try:\n",
    "                            useful_votes.append(vote.get_text().split(' ')[1])\n",
    "                        except:\n",
    "                            useful_votes.append(0)\n",
    "                    elif vote.get_text().split(' ')[0] == 'Funny':\n",
    "                        try:\n",
    "                            funny_votes.append(vote.get_text().split(' ')[1])\n",
    "                        except:\n",
    "                            funny_votes.append(0)\n",
    "                    else:\n",
    "                        try:\n",
    "                            cool_votes.append(vote.get_text().split(' ')[1])\n",
    "                        except:\n",
    "                            cool_votes.append(0)    \n",
    "                review_vars1 = [review_names, review_ratings, review_dates, \n",
    "                                review_texts, reviewer_city, reviewer_state,\n",
    "                                useful_votes, funny_votes, cool_votes]\n",
    "\n",
    "\n",
    "                #### Getting the inconsistent variables ####\n",
    "                n_photos = []\n",
    "                r_friends = []\n",
    "                r_reviews = []\n",
    "                r_photos = []\n",
    "                elite_tag = []\n",
    "\n",
    "                raw_reviews = soup.select('.review__373c0__13kpL')\n",
    "                for raw_review in raw_reviews:\n",
    "\n",
    "                    ### Number of Photos in the review ###\n",
    "                    raw_n_photos = raw_review.select('.margin-b0-5__373c0__Fo75u:nth-child(1) .css-166la90')\n",
    "                    if len(raw_n_photos) == 0:\n",
    "                        n_photos.append(0)\n",
    "                    else:\n",
    "                        try:\n",
    "                            n_photos.append(int(raw_n_photos[0].string.split()[0]))\n",
    "                        except:\n",
    "                            n_photos.append(0)\n",
    "\n",
    "                    ### Profile tags (# Friends, # Reviews, # Pictures) ###   \n",
    "                    raw_tags = raw_review.select('.arrange-unit-fill__373c0__17z0h .border-color--default__373c0__2oFDT .border-color--default__373c0__2oFDT .border-color--default__373c0__2oFDT .border-color--default__373c0__2oFDT .border-color--default__373c0__2oFDT .border-color--default__373c0__2oFDT .css-1dgkz3l')\n",
    "                    r_friends.append(int(raw_tags[0].string))\n",
    "                    r_reviews.append(int(raw_tags[1].string))\n",
    "                    try:\n",
    "                        r_photos.append(int(raw_tags[2].string))\n",
    "                    except:\n",
    "                        r_photos.append(0)\n",
    "\n",
    "                    ### The \"Yelp Elite Squad\" tag ###\n",
    "                    raw_elite_tag = raw_review.select('.css-1hx6l2b')\n",
    "                    if len(raw_elite_tag) == 0:\n",
    "                        elite_tag.append(0)\n",
    "                    else:\n",
    "                        elite_tag.append(1)\n",
    "\n",
    "                #### Combining all variables into 1 dataset ####\n",
    "\n",
    "                business_names = [business_name] * len(review_names)\n",
    "                business_urls = [business_url] * len(review_names)\n",
    "                mean_ratings = [mean_rating] * len(review_names)\n",
    "                total_reviews_all = [total_reviews] * len(review_names)\n",
    "                claimed_status_all = [claimed_status] * len(review_names)\n",
    "                price_cat_all = [price_cat] * len(review_names)\n",
    "                categories_all = [categories] * len(review_names)\n",
    "                tot_n_photos_all = [tot_n_photos] * len(review_names)\n",
    "                health_score_all = [health_score] * len(review_names)\n",
    "                cov_updates_all = [cov_updates] * len(review_names)\n",
    "                st_address_all = [st_address] * len(review_names)\n",
    "\n",
    "                review_data = list(zip(business_names, business_urls, mean_ratings, total_reviews_all, \n",
    "                                       claimed_status_all,price_cat_all, categories_all, \n",
    "                                       tot_n_photos_all, health_score_all, cov_updates_all,\n",
    "                                       st_address_all, review_names, review_ratings, review_dates, \n",
    "                                       review_texts, n_photos, r_friends, r_reviews, \n",
    "                                       r_photos, elite_tag))\n",
    "\n",
    "                df_reviews = pd.DataFrame(data=review_data,\n",
    "                                          columns = ['name_business', 'url', 'mean_rating', 'total_reviews', \n",
    "                                                     'claimed_status','price_cat', 'categories', \n",
    "                                                     'tot_n_photos', 'health_score', 'cov_updates', \n",
    "                                                     'st_address','username', 'rating', \n",
    "                                                     'date_review', 'text', 'n_photos',\n",
    "                                                     'r_friends', 'r_reviews', 'r_photos',\n",
    "                                                     'elite_tag'])\n",
    "\n",
    "                with open('memphis_reviews.csv', 'a', newline='', encoding='utf-8') as f:\n",
    "                    df_reviews.to_csv(f, index=False, header=False)\n",
    "\n",
    "                print('{} review page {} has been scraped'.format(row[1]['name'], str(i)))\n",
    "                time.sleep(2)\n",
    "                \n",
    "            except:\n",
    "                print('Error in stage 2')\n",
    "\n",
    "        print('{} has been scraped'.format(row[1]['name']))    \n",
    "        time.sleep(2)\n",
    "    \n",
    "    except:\n",
    "        print('Error in stage 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test version without try/except to find the errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = requests.get('https://www.yelp.com/biz/elwoods-shack-memphis')\n",
    "soup = BeautifulSoup(html.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test https://www.yelp.com/biz/magnolia-and-may-memphis\n",
      "q\n",
      "q\n",
      "q\n"
     ]
    }
   ],
   "source": [
    "#### Get the business name and url ####\n",
    "business_name = 'Test'\n",
    "business_url = 'https://www.yelp.com/biz/magnolia-and-may-memphis'\n",
    "print(business_name, business_url)\n",
    "\n",
    "#### Request the page ####\n",
    "html = requests.get(business_url)\n",
    "soup = BeautifulSoup(html.content, 'html.parser')\n",
    "\n",
    "#### Average Rating ####\n",
    "try:\n",
    "    mean_rating = soup.select('.i-stars--large-5__373c0__1GcGD')\n",
    "    mean_rating = float(mean_rating[0].attrs['aria-label'].split(' ')[0])\n",
    "except:\n",
    "    try:\n",
    "        mean_rating = soup.select('.i-stars--large-4-half__373c0__2lYkD')\n",
    "        mean_rating = float(mean_rating[0].attrs['aria-label'].split(' ')[0])\n",
    "    except:\n",
    "        try:\n",
    "            mean_rating = soup.select('.i-stars--large-4__373c0__1d6HV')\n",
    "            mean_rating = float(mean_rating[0].attrs['aria-label'].split(' ')[0])\n",
    "        except:\n",
    "            try:\n",
    "                mean_rating = soup.select('.i-stars--large-3-half__373c0__2z4jR')\n",
    "                mean_rating = float(mean_rating[0].attrs['aria-label'].split(' ')[0])\n",
    "            except:\n",
    "                try: \n",
    "                    mean_rating = soup.select('.i-stars--large-3__373c0__3_Jon')\n",
    "                    mean_rating = float(mean_rating[0].attrs['aria-label'].split(' ')[0])\n",
    "                except:\n",
    "                    mean_rating = 0\n",
    "\n",
    "\n",
    "# Total Number of Reviews\n",
    "total_reviews = soup.select('.arrange-unit-fill__373c0__17z0h.nowrap__373c0__1_N1j .css-bq71j2')\n",
    "total_reviews = int(total_reviews[0].string.split(' ')[0])\n",
    "\n",
    "# Claimed Status\n",
    "try:\n",
    "    claimed_status = soup.select('.css-1xxismk .css-166la90')[0].string\n",
    "    claimed_status = 0\n",
    "except:\n",
    "    claimed_status = 1\n",
    "\n",
    "# Price Category\n",
    "try:\n",
    "    price_cat = soup.select('.margin-r1__373c0__zyKmV+ .border-color--default__373c0__2oFDT .css-1xxismk')\n",
    "    price_cat = len((price_cat[0].text).strip())\n",
    "except:\n",
    "    price_cat = None\n",
    "\n",
    "# Restaurant Categories\n",
    "categories = soup.select('.css-bq71j2 .css-166la90')\n",
    "categories = [x.string for x in categories]\n",
    "\n",
    "# Total number of Photos\n",
    "try:\n",
    "    tot_n_photos = soup.select('.css-1fepc68 .css-ardur')\n",
    "    tot_n_photos = int(tot_n_photos[0].string.split(' ')[1])\n",
    "except:\n",
    "    tot_n_photos = 0\n",
    "\n",
    "# Health Score\n",
    "try:\n",
    "    health_score = soup.select('.label-spacing-v2__373c0__PBYkt')\n",
    "    health_score = int(health_score[0].string.split(' ')[0])\n",
    "except:\n",
    "    health_score = None\n",
    "\n",
    "# Covid Updates\n",
    "cov_updates = soup.select('.margin-b1__373c0__1khoT .css-1h1j0y3')\n",
    "cov_updates = [x.string for x in cov_updates]\n",
    "\n",
    "# Street Address\n",
    "st_address = soup.select('.css-1bmgof7 .raw__373c0__3rcx7')\n",
    "st_address = st_address[0].string    \n",
    "\n",
    "business_info = [business_name, business_url, total_reviews, claimed_status,\n",
    "                     price_cat, categories, tot_n_photos, health_score, cov_updates, st_address]\n",
    "\n",
    "#### Get the data for multiple review pages ####\n",
    "#### Set the number of review pages that should be scraped ####\n",
    "\n",
    "for i in range(3):\n",
    "    url2 = business_url + '?start=' + str(i * 10)\n",
    "    html = requests.get(url2)\n",
    "    soup = BeautifulSoup(html.content, 'html.parser')\n",
    "\n",
    "    #### Names\n",
    "    names = soup.select('.css-m6anxm .css-166la90')\n",
    "    review_names = []\n",
    "\n",
    "    for name in names:\n",
    "        review_names.append(name.string)\n",
    "\n",
    "    #### Ratings\n",
    "    ratings = soup.select('.margin-b1-5__373c0__2Wblx .overflow--hidden__373c0__2B0kz')\n",
    "\n",
    "    review_ratings = []\n",
    "\n",
    "    for rating in ratings:\n",
    "        review_ratings.append(float(re.sub(' star rating', \n",
    "                                           '',rating.attrs['aria-label'])))\n",
    "    #### Dates\n",
    "    dates = soup.select('.margin-b1-5__373c0__2Wblx .css-e81eai')\n",
    "\n",
    "    review_dates = []\n",
    "\n",
    "    for date in dates:\n",
    "        review_dates.append(date.string)\n",
    "\n",
    "    #### Text\n",
    "    texts = soup.select('.css-n6i4z7 .raw__373c0__3rcx7')\n",
    "    review_texts = []\n",
    "\n",
    "    for text in texts:\n",
    "        review_texts.append(text.get_text())\n",
    "\n",
    "    #### Reviewer's Location\n",
    "    reviewer_loc = soup.select('.arrange-unit-fill__373c0__17z0h .border-color--default__373c0__2oFDT .border-color--default__373c0__2oFDT .border-color--default__373c0__2oFDT .css-n6i4z7')\n",
    "\n",
    "    reviewer_city = []\n",
    "    reviewer_state = []\n",
    "\n",
    "    for loc in reviewer_loc:\n",
    "        if len(loc.get_text().split(',')) == 2:\n",
    "            reviewer_city.append(loc.get_text().split(',')[0])\n",
    "            reviewer_state.append(loc.get_text().split(',')[1])\n",
    "        elif len(loc.get_text().split(',')) == 3:\n",
    "            reviewer_city.append(loc.get_text().split(',')[1])\n",
    "            reviewer_state.append(loc.get_text().split(',')[2])\n",
    "        else:\n",
    "            reviewer_city.append(None)\n",
    "            reviewer_city.append(None)    \n",
    "\n",
    "    #### Votes\n",
    "    votes = soup.select('.arrange-unit-fill__373c0__17z0h .display--inline__373c0__1DbOG .css-1ha1j8d')\n",
    "\n",
    "    useful_votes = []\n",
    "    funny_votes = []\n",
    "    cool_votes = []\n",
    "\n",
    "    for vote in votes:\n",
    "        if vote.get_text().split(' ')[0] == 'Useful':\n",
    "            try:\n",
    "                useful_votes.append(vote.get_text().split(' ')[1])\n",
    "            except:\n",
    "                useful_votes.append(0)\n",
    "        elif vote.get_text().split(' ')[0] == 'Funny':\n",
    "            try:\n",
    "                funny_votes.append(vote.get_text().split(' ')[1])\n",
    "            except:\n",
    "                funny_votes.append(0)\n",
    "        else:\n",
    "            try:\n",
    "                cool_votes.append(vote.get_text().split(' ')[1])\n",
    "            except:\n",
    "                cool_votes.append(0)    \n",
    "    review_vars1 = [review_names, review_ratings, review_dates, \n",
    "                    review_texts, reviewer_city, reviewer_state,\n",
    "                    useful_votes, funny_votes, cool_votes]\n",
    "\n",
    "\n",
    "    #### Getting the inconsistent variables ####\n",
    "    n_photos = []\n",
    "    r_friends = []\n",
    "    r_reviews = []\n",
    "    r_photos = []\n",
    "    elite_tag = []\n",
    "\n",
    "    raw_reviews = soup.select('.review__373c0__13kpL')\n",
    "    for raw_review in raw_reviews:\n",
    "\n",
    "        ### Number of Photos in the review ###\n",
    "        raw_n_photos = raw_review.select('.margin-b0-5__373c0__Fo75u:nth-child(1) .css-166la90')\n",
    "        if len(raw_n_photos) == 0:\n",
    "            n_photos.append(0)\n",
    "        else:\n",
    "            try:\n",
    "                n_photos.append(int(raw_n_photos[0].string.split()[0]))\n",
    "            except:\n",
    "                n_photos.append(0)\n",
    "\n",
    "        ### Profile tags (# Friends, # Reviews, # Pictures) ###   \n",
    "        raw_tags = raw_review.select('.arrange-unit-fill__373c0__17z0h .border-color--default__373c0__2oFDT .border-color--default__373c0__2oFDT .border-color--default__373c0__2oFDT .border-color--default__373c0__2oFDT .border-color--default__373c0__2oFDT .border-color--default__373c0__2oFDT .css-1dgkz3l')\n",
    "        r_friends.append(int(raw_tags[0].string))\n",
    "        r_reviews.append(int(raw_tags[1].string))\n",
    "        try:\n",
    "            r_photos.append(int(raw_tags[2].string))\n",
    "        except:\n",
    "            r_photos.append(0)\n",
    "\n",
    "        ### The \"Yelp Elite Squad\" tag ###\n",
    "        raw_elite_tag = raw_review.select('.css-1hx6l2b')\n",
    "        if len(raw_elite_tag) == 0:\n",
    "            elite_tag.append(0)\n",
    "        else:\n",
    "            elite_tag.append(1)\n",
    "\n",
    "    #### Combining all variables into 1 dataset ####\n",
    "\n",
    "    business_names = [business_name] * len(review_names)\n",
    "    business_urls = [business_url] * len(review_names)\n",
    "    mean_ratings = [mean_rating] * len(review_names)\n",
    "    total_reviews_all = [total_reviews] * len(review_names)\n",
    "    claimed_status_all = [claimed_status] * len(review_names)\n",
    "    price_cat_all = [price_cat] * len(review_names)\n",
    "    categories_all = [categories] * len(review_names)\n",
    "    tot_n_photos_all = [tot_n_photos] * len(review_names)\n",
    "    health_score_all = [health_score] * len(review_names)\n",
    "    cov_updates_all = [cov_updates] * len(review_names)\n",
    "    st_address_all = [st_address] * len(review_names)\n",
    "\n",
    "    review_data = list(zip(business_names, business_urls, mean_ratings, total_reviews_all, \n",
    "                           claimed_status_all,price_cat_all, categories_all, \n",
    "                           tot_n_photos_all, health_score_all, cov_updates_all,\n",
    "                           st_address_all, review_names, review_ratings, review_dates, \n",
    "                           review_texts, n_photos, r_friends, r_reviews, \n",
    "                           r_photos, elite_tag))\n",
    "\n",
    "    df_reviews = pd.DataFrame(data=review_data,\n",
    "                              columns = ['name_business', 'url', 'mean_rating', 'total_reviews', \n",
    "                                         'claimed_status','price_cat', 'categories', \n",
    "                                         'tot_n_photos', 'health_score', 'cov_updates', \n",
    "                                         'st_address','username', 'rating', \n",
    "                                         'date_review', 'text', 'n_photos',\n",
    "                                         'r_friends', 'r_reviews', 'r_photos',\n",
    "                                         'elite_tag'])\n",
    "\n",
    "    with open('memphis_reviews.csv', 'a', newline='', encoding='utf-8') as f:\n",
    "        df_reviews.to_csv(f, index=False, header=False)\n",
    "\n",
    "    print('q')\n",
    "    #time.sleep(randint(4, 12))\n",
    "\n",
    "\n",
    "#print('{} has been scraped'.format(row[1]['name']))    \n",
    "#time.sleep(randint(4, 12))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Magnolia &amp; May</td>\n",
       "      <td>https://www.yelp.com/biz/magnolia-and-may-memphis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Elwood’s Shack</td>\n",
       "      <td>https://www.yelp.com/biz/elwoods-shack-memphis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>ECCO on Overton Park</td>\n",
       "      <td>https://www.yelp.com/biz/ecco-on-overton-park-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Maximo’s On Broad</td>\n",
       "      <td>https://www.yelp.com/biz/maximos-on-broad-memphis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Restaurant Iris</td>\n",
       "      <td>https://www.yelp.com/biz/restaurant-iris-memphis</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    name                                                url\n",
       "18        Magnolia & May  https://www.yelp.com/biz/magnolia-and-may-memphis\n",
       "19        Elwood’s Shack     https://www.yelp.com/biz/elwoods-shack-memphis\n",
       "20  ECCO on Overton Park  https://www.yelp.com/biz/ecco-on-overton-park-...\n",
       "21     Maximo’s On Broad  https://www.yelp.com/biz/maximos-on-broad-memphis\n",
       "22       Restaurant Iris   https://www.yelp.com/biz/restaurant-iris-memphis"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_small = df[18:23]\n",
    "df_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Magnolia & May https://www.yelp.com/biz/magnolia-and-may-memphis\n",
      "Magnolia & May review page 0 has been scraped\n",
      "Magnolia & May has been scraped\n",
      "Elwood’s Shack https://www.yelp.com/biz/elwoods-shack-memphis\n",
      "Elwood’s Shack review page 0 has been scraped\n",
      "Elwood’s Shack has been scraped\n",
      "ECCO on Overton Park https://www.yelp.com/biz/ecco-on-overton-park-memphis\n",
      "ECCO on Overton Park review page 0 has been scraped\n",
      "ECCO on Overton Park has been scraped\n",
      "Maximo’s On Broad https://www.yelp.com/biz/maximos-on-broad-memphis\n",
      "Maximo’s On Broad review page 0 has been scraped\n",
      "Maximo’s On Broad has been scraped\n",
      "Restaurant Iris https://www.yelp.com/biz/restaurant-iris-memphis\n",
      "Restaurant Iris review page 0 has been scraped\n",
      "Restaurant Iris has been scraped\n"
     ]
    }
   ],
   "source": [
    "for row in df_small.iterrows():\n",
    "    #### Get the business name and url ####\n",
    "    business_name = row[1]['name']\n",
    "    business_url = row[1]['url']\n",
    "    print(business_name, business_url)\n",
    "\n",
    "    #### Request the page ####\n",
    "    html = requests.get(business_url)\n",
    "    soup = BeautifulSoup(html.content, 'html.parser')\n",
    "\n",
    "    #### Average Rating ####\n",
    "    try:\n",
    "        mean_rating = soup.select('.i-stars--large-5__373c0__1GcGD')\n",
    "        mean_rating = float(mean_rating[0].attrs['aria-label'].split(' ')[0])\n",
    "    except:\n",
    "        try:\n",
    "            mean_rating = soup.select('.i-stars--large-4-half__373c0__2lYkD')\n",
    "            mean_rating = float(mean_rating[0].attrs['aria-label'].split(' ')[0])\n",
    "        except:\n",
    "            try:\n",
    "                mean_rating = soup.select('.i-stars--large-4__373c0__1d6HV')\n",
    "                mean_rating = float(mean_rating[0].attrs['aria-label'].split(' ')[0])\n",
    "            except:\n",
    "                try:\n",
    "                    mean_rating = soup.select('.i-stars--large-3-half__373c0__2z4jR')\n",
    "                    mean_rating = float(mean_rating[0].attrs['aria-label'].split(' ')[0])\n",
    "                except:\n",
    "                    try: \n",
    "                        mean_rating = soup.select('.i-stars--large-3__373c0__3_Jon')\n",
    "                        mean_rating = float(mean_rating[0].attrs['aria-label'].split(' ')[0])\n",
    "                    except:\n",
    "                        mean_rating = 0\n",
    "\n",
    "\n",
    "    # Total Number of Reviews\n",
    "    total_reviews = soup.select('.arrange-unit-fill__373c0__17z0h.nowrap__373c0__1_N1j .css-bq71j2')\n",
    "    total_reviews = int(total_reviews[0].string.split(' ')[0])\n",
    "\n",
    "    # Claimed Status\n",
    "    try:\n",
    "        claimed_status = soup.select('.css-1xxismk .css-166la90')[0].string\n",
    "        claimed_status = 0\n",
    "    except:\n",
    "        claimed_status = 1\n",
    "\n",
    "    # Price Category\n",
    "    try:\n",
    "        price_cat = soup.select('.margin-r1__373c0__zyKmV+ .border-color--default__373c0__2oFDT .css-1xxismk')\n",
    "        price_cat = len((price_cat[0].text).strip())\n",
    "    except:\n",
    "        price_cat = None\n",
    "\n",
    "    # Restaurant Categories\n",
    "    categories = soup.select('.css-bq71j2 .css-166la90')\n",
    "    categories = [x.string for x in categories]\n",
    "\n",
    "    # Total number of Photos\n",
    "    try:\n",
    "        tot_n_photos = soup.select('.css-1fepc68 .css-ardur')\n",
    "        tot_n_photos = int(tot_n_photos[0].string.split(' ')[1])\n",
    "    except:\n",
    "        tot_n_photos = 0\n",
    "\n",
    "    # Health Score\n",
    "    try:\n",
    "        health_score = soup.select('.label-spacing-v2__373c0__PBYkt')\n",
    "        health_score = int(health_score[0].string.split(' ')[0])\n",
    "    except:\n",
    "        health_score = None\n",
    "\n",
    "    # Covid Updates\n",
    "    cov_updates = soup.select('.margin-b1__373c0__1khoT .css-1h1j0y3')\n",
    "    cov_updates = [x.string for x in cov_updates]\n",
    "\n",
    "    # Street Address\n",
    "    st_address = soup.select('.css-1bmgof7 .raw__373c0__3rcx7')\n",
    "    st_address = st_address[0].string    \n",
    "\n",
    "    business_info = [business_name, business_url, total_reviews, claimed_status,\n",
    "                         price_cat, categories, tot_n_photos, health_score, cov_updates, st_address]\n",
    "\n",
    "    #### Get the data for multiple review pages ####\n",
    "    #### Set the number of review pages that should be scraped ####\n",
    "\n",
    "    for i in range(1):\n",
    "        try:\n",
    "            url2 = business_url + '?start=' + str(i * 10)\n",
    "            html = requests.get(url2)\n",
    "            soup = BeautifulSoup(html.content, 'html.parser')\n",
    "\n",
    "            #### Names\n",
    "            names = soup.select('.css-m6anxm .css-166la90')\n",
    "            review_names = []\n",
    "\n",
    "            for name in names:\n",
    "                review_names.append(name.string)\n",
    "\n",
    "            #### Ratings\n",
    "            ratings = soup.select('.margin-b1-5__373c0__2Wblx .overflow--hidden__373c0__2B0kz')\n",
    "\n",
    "            review_ratings = []\n",
    "\n",
    "            for rating in ratings:\n",
    "                review_ratings.append(float(re.sub(' star rating', \n",
    "                                                   '',rating.attrs['aria-label'])))\n",
    "            #### Dates\n",
    "            dates = soup.select('.margin-b1-5__373c0__2Wblx .css-e81eai')\n",
    "\n",
    "            review_dates = []\n",
    "\n",
    "            for date in dates:\n",
    "                review_dates.append(date.string)\n",
    "\n",
    "            #### Text\n",
    "            texts = soup.select('.css-n6i4z7 .raw__373c0__3rcx7')\n",
    "            review_texts = []\n",
    "\n",
    "            for text in texts:\n",
    "                review_texts.append(text.get_text())\n",
    "\n",
    "            #### Reviewer's Location\n",
    "            reviewer_loc = soup.select('.arrange-unit-fill__373c0__17z0h .border-color--default__373c0__2oFDT .border-color--default__373c0__2oFDT .border-color--default__373c0__2oFDT .css-n6i4z7')\n",
    "\n",
    "            reviewer_city = []\n",
    "            reviewer_state = []\n",
    "\n",
    "            for loc in reviewer_loc:\n",
    "                if len(loc.get_text().split(',')) == 2:\n",
    "                    reviewer_city.append(loc.get_text().split(',')[0])\n",
    "                    reviewer_state.append(loc.get_text().split(',')[1])\n",
    "                elif len(loc.get_text().split(',')) == 3:\n",
    "                    reviewer_city.append(loc.get_text().split(',')[1])\n",
    "                    reviewer_state.append(loc.get_text().split(',')[2])\n",
    "                else:\n",
    "                    reviewer_city.append(None)\n",
    "                    reviewer_city.append(None)    \n",
    "\n",
    "            #### Votes\n",
    "            votes = soup.select('.arrange-unit-fill__373c0__17z0h .display--inline__373c0__1DbOG .css-1ha1j8d')\n",
    "\n",
    "            useful_votes = []\n",
    "            funny_votes = []\n",
    "            cool_votes = []\n",
    "\n",
    "            for vote in votes:\n",
    "                if vote.get_text().split(' ')[0] == 'Useful':\n",
    "                    try:\n",
    "                        useful_votes.append(vote.get_text().split(' ')[1])\n",
    "                    except:\n",
    "                        useful_votes.append(0)\n",
    "                elif vote.get_text().split(' ')[0] == 'Funny':\n",
    "                    try:\n",
    "                        funny_votes.append(vote.get_text().split(' ')[1])\n",
    "                    except:\n",
    "                        funny_votes.append(0)\n",
    "                else:\n",
    "                    try:\n",
    "                        cool_votes.append(vote.get_text().split(' ')[1])\n",
    "                    except:\n",
    "                        cool_votes.append(0)    \n",
    "            review_vars1 = [review_names, review_ratings, review_dates, \n",
    "                            review_texts, reviewer_city, reviewer_state,\n",
    "                            useful_votes, funny_votes, cool_votes]\n",
    "\n",
    "\n",
    "            #### Getting the inconsistent variables ####\n",
    "            n_photos = []\n",
    "            r_friends = []\n",
    "            r_reviews = []\n",
    "            r_photos = []\n",
    "            elite_tag = []\n",
    "\n",
    "            raw_reviews = soup.select('.review__373c0__13kpL')\n",
    "            for raw_review in raw_reviews:\n",
    "\n",
    "                ### Number of Photos in the review ###\n",
    "                raw_n_photos = raw_review.select('.margin-b0-5__373c0__Fo75u:nth-child(1) .css-166la90')\n",
    "                if len(raw_n_photos) == 0:\n",
    "                    n_photos.append(0)\n",
    "                else:\n",
    "                    try:\n",
    "                        n_photos.append(int(raw_n_photos[0].string.split()[0]))\n",
    "                    except:\n",
    "                        n_photos.append(0)\n",
    "\n",
    "                ### Profile tags (# Friends, # Reviews, # Pictures) ###   \n",
    "                raw_tags = raw_review.select('.arrange-unit-fill__373c0__17z0h .border-color--default__373c0__2oFDT .border-color--default__373c0__2oFDT .border-color--default__373c0__2oFDT .border-color--default__373c0__2oFDT .border-color--default__373c0__2oFDT .border-color--default__373c0__2oFDT .css-1dgkz3l')\n",
    "                r_friends.append(int(raw_tags[0].string))\n",
    "                r_reviews.append(int(raw_tags[1].string))\n",
    "                try:\n",
    "                    r_photos.append(int(raw_tags[2].string))\n",
    "                except:\n",
    "                    r_photos.append(0)\n",
    "\n",
    "                ### The \"Yelp Elite Squad\" tag ###\n",
    "                raw_elite_tag = raw_review.select('.css-1hx6l2b')\n",
    "                if len(raw_elite_tag) == 0:\n",
    "                    elite_tag.append(0)\n",
    "                else:\n",
    "                    elite_tag.append(1)\n",
    "\n",
    "            #### Combining all variables into 1 dataset ####\n",
    "\n",
    "            business_names = [business_name] * len(review_names)\n",
    "            business_urls = [business_url] * len(review_names)\n",
    "            mean_ratings = [mean_rating] * len(review_names)\n",
    "            total_reviews_all = [total_reviews] * len(review_names)\n",
    "            claimed_status_all = [claimed_status] * len(review_names)\n",
    "            price_cat_all = [price_cat] * len(review_names)\n",
    "            categories_all = [categories] * len(review_names)\n",
    "            tot_n_photos_all = [tot_n_photos] * len(review_names)\n",
    "            health_score_all = [health_score] * len(review_names)\n",
    "            cov_updates_all = [cov_updates] * len(review_names)\n",
    "            st_address_all = [st_address] * len(review_names)\n",
    "\n",
    "            review_data = list(zip(business_names, business_urls, mean_ratings, total_reviews_all, \n",
    "                                   claimed_status_all,price_cat_all, categories_all, \n",
    "                                   tot_n_photos_all, health_score_all, cov_updates_all,\n",
    "                                   st_address_all, review_names, review_ratings, review_dates, \n",
    "                                   review_texts, n_photos, r_friends, r_reviews, \n",
    "                                   r_photos, elite_tag))\n",
    "\n",
    "            df_reviews = pd.DataFrame(data=review_data,\n",
    "                                      columns = ['name_business', 'url', 'mean_rating', 'total_reviews', \n",
    "                                                 'claimed_status','price_cat', 'categories', \n",
    "                                                 'tot_n_photos', 'health_score', 'cov_updates', \n",
    "                                                 'st_address','username', 'rating', \n",
    "                                                 'date_review', 'text', 'n_photos',\n",
    "                                                 'r_friends', 'r_reviews', 'r_photos',\n",
    "                                                 'elite_tag'])\n",
    "\n",
    "            with open('memphis_reviews.csv', 'a', newline='', encoding='utf-8') as f:\n",
    "                df_reviews.to_csv(f, index=False, header=False)\n",
    "\n",
    "            print('{} review page {} has been scraped'.format(row[1]['name'], str(i)))\n",
    "            time.sleep(randint(4, 12))\n",
    "\n",
    "        except:\n",
    "            print('Error in stage 2')\n",
    "\n",
    "    print('{} has been scraped'.format(row[1]['name']))    \n",
    "    time.sleep(randint(4, 12))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}