{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('memphis_restaurants.csv')\n",
    "df = df.drop(['Unnamed: 0'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Second Line https://www.yelp.com/biz/the-second-line-memphis\n",
      "The Second Line review page 0 has been scraped\n",
      "The Second Line review page 1 has been scraped\n",
      "The Second Line has been scraped\n"
     ]
    }
   ],
   "source": [
    "for row in df.iterrows():\n",
    "    try:\n",
    "        #### Get the business name and url ####\n",
    "        business_name = row[1]['name']\n",
    "        business_url = row[1]['url']\n",
    "        print(business_name, business_url)\n",
    "\n",
    "        #### Request the page ####\n",
    "        html = requests.get(business_url)\n",
    "        soup = BeautifulSoup(html.content, 'html.parser')\n",
    "        \n",
    "        #### Average Rating ####\n",
    "        try:\n",
    "            mean_rating = soup.select('.i-stars--large-5__373c0__1GcGD')\n",
    "            mean_rating = float(mean_rating[0].attrs['aria-label'].split(' ')[0])\n",
    "        except:\n",
    "            try:\n",
    "                mean_rating = soup.select('.i-stars--large-4-half__373c0__2lYkD')\n",
    "                mean_rating = float(mean_rating[0].attrs['aria-label'].split(' ')[0])\n",
    "            except:\n",
    "                try:\n",
    "                    mean_rating = soup.select('.i-stars--large-4__373c0__1d6HV')\n",
    "                    mean_rating = float(mean_rating[0].attrs['aria-label'].split(' ')[0])\n",
    "                except:\n",
    "                    try:\n",
    "                        mean_rating = soup.select('.i-stars--large-3-half__373c0__2z4jR')\n",
    "                        mean_rating = float(mean_rating[0].attrs['aria-label'].split(' ')[0])\n",
    "                    except:\n",
    "                        try: \n",
    "                            mean_rating = soup.select('.i-stars--large-3__373c0__3_Jon')\n",
    "                            mean_rating = float(mean_rating[0].attrs['aria-label'].split(' ')[0])\n",
    "                        except:\n",
    "                            mean_rating = 0\n",
    "\n",
    "\n",
    "        # Total Number of Reviews\n",
    "        total_reviews = soup.select('.arrange-unit-fill__373c0__17z0h.nowrap__373c0__1_N1j .css-bq71j2')\n",
    "        total_reviews = int(total_reviews[0].string.split(' ')[0])\n",
    "\n",
    "        # Claimed Status\n",
    "        try:\n",
    "            claimed_status = soup.select('.css-1xxismk .css-166la90')[0].string\n",
    "            claimed_status = 0\n",
    "        except:\n",
    "            claimed_status = 1\n",
    "\n",
    "        # Price Category\n",
    "        try:\n",
    "            price_cat = soup.select('.margin-r1__373c0__zyKmV+ .border-color--default__373c0__2oFDT .css-1xxismk')\n",
    "            price_cat = len((price_cat[0].text).strip())\n",
    "        except:\n",
    "            price_cat = None\n",
    "            \n",
    "        # Restaurant Categories\n",
    "        categories = soup.select('.css-bq71j2 .css-166la90')\n",
    "        categories = [x.string for x in categories]\n",
    "\n",
    "        # Total number of Photos\n",
    "        try:\n",
    "            tot_n_photos = soup.select('.css-1fepc68 .css-ardur')\n",
    "            tot_n_photos = int(tot_n_photos[0].string.split(' ')[1])\n",
    "        except:\n",
    "            tot_n_photos = 0\n",
    "            \n",
    "        # Health Score\n",
    "        try:\n",
    "            health_score = soup.select('.label-spacing-v2__373c0__PBYkt')\n",
    "            health_score = int(health_score[0].string.split(' ')[0])\n",
    "        except:\n",
    "            health_score = None\n",
    "            \n",
    "        # Covid Updates\n",
    "        cov_updates = soup.select('.margin-b1__373c0__1khoT .css-1h1j0y3')\n",
    "        cov_updates = [x.string for x in cov_updates]\n",
    "\n",
    "        # Street Address\n",
    "        st_address = soup.select('.css-1bmgof7 .raw__373c0__3rcx7')\n",
    "        st_address = st_address[0].string    \n",
    "\n",
    "        business_info = [business_name, business_url, total_reviews, claimed_status,\n",
    "                             price_cat, categories, tot_n_photos, health_score, cov_updates, st_address]\n",
    "\n",
    "        #### Get the data for multiple review pages ####\n",
    "        #### Set the number of review pages that should be scraped ####\n",
    "\n",
    "        for i in range(3):\n",
    "            try:\n",
    "                url2 = business_url + '?start=' + str(i * 10)\n",
    "                html = requests.get(url2)\n",
    "                soup = BeautifulSoup(html.content, 'html.parser')\n",
    "\n",
    "                #### Names\n",
    "                names = soup.select('.css-m6anxm .css-166la90')\n",
    "                review_names = []\n",
    "\n",
    "                for name in names:\n",
    "                    review_names.append(name.string)\n",
    "\n",
    "                #### Ratings\n",
    "                ratings = soup.select('.margin-b1-5__373c0__2Wblx .overflow--hidden__373c0__2B0kz')\n",
    "\n",
    "                review_ratings = []\n",
    "\n",
    "                for rating in ratings:\n",
    "                    review_ratings.append(float(re.sub(' star rating', \n",
    "                                                       '',rating.attrs['aria-label'])))\n",
    "                #### Dates\n",
    "                dates = soup.select('.margin-b1-5__373c0__2Wblx .css-e81eai')\n",
    "\n",
    "                review_dates = []\n",
    "\n",
    "                for date in dates:\n",
    "                    review_dates.append(date.string)\n",
    "\n",
    "                #### Text\n",
    "                texts = soup.select('.css-n6i4z7 .raw__373c0__3rcx7')\n",
    "                review_texts = []\n",
    "\n",
    "                for text in texts:\n",
    "                    review_texts.append(text.get_text())\n",
    "\n",
    "                #### Reviewer's Location\n",
    "                reviewer_loc = soup.select('.arrange-unit-fill__373c0__17z0h .border-color--default__373c0__2oFDT .border-color--default__373c0__2oFDT .border-color--default__373c0__2oFDT .css-n6i4z7')\n",
    "\n",
    "                reviewer_city = []\n",
    "                reviewer_state = []\n",
    "\n",
    "                for loc in reviewer_loc:\n",
    "                    if len(loc.get_text().split(',')) == 2:\n",
    "                        reviewer_city.append(loc.get_text().split(',')[0])\n",
    "                        reviewer_state.append(loc.get_text().split(',')[1])\n",
    "                    elif len(loc.get_text().split(',')) == 3:\n",
    "                        reviewer_city.append(loc.get_text().split(',')[1])\n",
    "                        reviewer_state.append(loc.get_text().split(',')[2])\n",
    "                    else:\n",
    "                        reviewer_city.append(None)\n",
    "                        reviewer_city.append(None)    \n",
    "\n",
    "                #### Votes\n",
    "                votes = soup.select('.arrange-unit-fill__373c0__17z0h .display--inline__373c0__1DbOG .css-1ha1j8d')\n",
    "\n",
    "                useful_votes = []\n",
    "                funny_votes = []\n",
    "                cool_votes = []\n",
    "\n",
    "                for vote in votes:\n",
    "                    if vote.get_text().split(' ')[0] == 'Useful':\n",
    "                        try:\n",
    "                            useful_votes.append(vote.get_text().split(' ')[1])\n",
    "                        except:\n",
    "                            useful_votes.append(0)\n",
    "                    elif vote.get_text().split(' ')[0] == 'Funny':\n",
    "                        try:\n",
    "                            funny_votes.append(vote.get_text().split(' ')[1])\n",
    "                        except:\n",
    "                            funny_votes.append(0)\n",
    "                    else:\n",
    "                        try:\n",
    "                            cool_votes.append(vote.get_text().split(' ')[1])\n",
    "                        except:\n",
    "                            cool_votes.append(0)    \n",
    "                review_vars1 = [review_names, review_ratings, review_dates, \n",
    "                                review_texts, reviewer_city, reviewer_state,\n",
    "                                useful_votes, funny_votes, cool_votes]\n",
    "\n",
    "\n",
    "                #### Getting the inconsistent variables ####\n",
    "                n_photos = []\n",
    "                r_friends = []\n",
    "                r_reviews = []\n",
    "                r_photos = []\n",
    "                elite_tag = []\n",
    "\n",
    "                raw_reviews = soup.select('.review__373c0__13kpL')\n",
    "                for raw_review in raw_reviews:\n",
    "\n",
    "                    ### Number of Photos in the review ###\n",
    "                    raw_n_photos = raw_review.select('.margin-b0-5__373c0__Fo75u:nth-child(1) .css-166la90')\n",
    "                    if len(raw_n_photos) == 0:\n",
    "                        n_photos.append(0)\n",
    "                    else:\n",
    "                        try:\n",
    "                            n_photos.append(int(raw_n_photos[0].string.split()[0]))\n",
    "                        except:\n",
    "                            n_photos.append(0)\n",
    "\n",
    "                    ### Profile tags (# Friends, # Reviews, # Pictures) ###   \n",
    "                    raw_tags = raw_review.select('.arrange-unit-fill__373c0__17z0h .border-color--default__373c0__2oFDT .border-color--default__373c0__2oFDT .border-color--default__373c0__2oFDT .border-color--default__373c0__2oFDT .border-color--default__373c0__2oFDT .border-color--default__373c0__2oFDT .css-1dgkz3l')\n",
    "                    r_friends.append(int(raw_tags[0].string))\n",
    "                    r_reviews.append(int(raw_tags[1].string))\n",
    "                    try:\n",
    "                        r_photos.append(int(raw_tags[2].string))\n",
    "                    except:\n",
    "                        r_photos.append(0)\n",
    "\n",
    "                    ### The \"Yelp Elite Squad\" tag ###\n",
    "                    raw_elite_tag = raw_review.select('.css-1hx6l2b')\n",
    "                    if len(raw_elite_tag) == 0:\n",
    "                        elite_tag.append(0)\n",
    "                    else:\n",
    "                        elite_tag.append(1)\n",
    "\n",
    "                #### Combining all variables into 1 dataset ####\n",
    "\n",
    "                business_names = [business_name] * len(review_names)\n",
    "                business_urls = [business_url] * len(review_names)\n",
    "                mean_ratings = [mean_rating] * len(review_names)\n",
    "                total_reviews_all = [total_reviews] * len(review_names)\n",
    "                claimed_status_all = [claimed_status] * len(review_names)\n",
    "                price_cat_all = [price_cat] * len(review_names)\n",
    "                categories_all = [categories] * len(review_names)\n",
    "                tot_n_photos_all = [tot_n_photos] * len(review_names)\n",
    "                health_score_all = [health_score] * len(review_names)\n",
    "                cov_updates_all = [cov_updates] * len(review_names)\n",
    "                st_address_all = [st_address] * len(review_names)\n",
    "\n",
    "                review_data = list(zip(business_names, business_urls, mean_ratings, total_reviews_all, \n",
    "                                       claimed_status_all,price_cat_all, categories_all, \n",
    "                                       tot_n_photos_all, health_score_all, cov_updates_all,\n",
    "                                       st_address_all, review_names, review_ratings, review_dates, \n",
    "                                       review_texts, n_photos, r_friends, r_reviews, \n",
    "                                       r_photos, elite_tag))\n",
    "\n",
    "                df_reviews = pd.DataFrame(data=review_data,\n",
    "                                          columns = ['name_business', 'url', 'mean_rating', 'total_reviews', \n",
    "                                                     'claimed_status','price_cat', 'categories', \n",
    "                                                     'tot_n_photos', 'health_score', 'cov_updates', \n",
    "                                                     'st_address','username', 'rating', \n",
    "                                                     'date_review', 'text', 'n_photos',\n",
    "                                                     'r_friends', 'r_reviews', 'r_photos',\n",
    "                                                     'elite_tag'])\n",
    "\n",
    "                with open('memphis_reviews.csv', 'a', newline='', encoding='utf-8') as f:\n",
    "                    df_reviews.to_csv(f, index=False, header=False)\n",
    "\n",
    "                print('{} review page {} has been scraped'.format(row[1]['name'], str(i)))\n",
    "                time.sleep(randint(4, 12))\n",
    "                \n",
    "            except:\n",
    "                print('Error in stage 2')\n",
    "\n",
    "        print('{} has been scraped'.format(row[1]['name']))    \n",
    "        time.sleep(randint(4, 12))\n",
    "    \n",
    "    except:\n",
    "        print('Error in stage 1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test version without try/except to find the errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Second Line https://www.yelp.com/biz/the-second-line-memphis\n"
     ]
    }
   ],
   "source": [
    "#### Get the business name and url ####\n",
    "business_name = 'The Second Line'\n",
    "business_url = 'https://www.yelp.com/biz/the-second-line-memphis'\n",
    "print(business_name, business_url)\n",
    "\n",
    "#### Request the page ####\n",
    "html = requests.get(business_url)\n",
    "soup = BeautifulSoup(html.content, 'html.parser')\n",
    "\n",
    "# Total Number of Reviews\n",
    "total_reviews = soup.select('.arrange-unit-fill__373c0__17z0h.nowrap__373c0__1_N1j .css-bq71j2')\n",
    "total_reviews = int(total_reviews[0].string.split(' ')[0])\n",
    "\n",
    "# Claimed Status\n",
    "try:\n",
    "    claimed_status = soup.select('.css-1xxismk .css-166la90')[0].string\n",
    "    claimed_status = 0\n",
    "except:\n",
    "    claimed_status = 1\n",
    "\n",
    "# Price Category\n",
    "try:\n",
    "    price_cat = soup.select('.margin-r1__373c0__zyKmV+ .border-color--default__373c0__2oFDT .css-1xxismk')\n",
    "    price_cat = len((price_cat[0].text).strip())\n",
    "except:\n",
    "    price_cat = None\n",
    "# Restaurant Categories\n",
    "categories = soup.select('.css-bq71j2 .css-166la90')\n",
    "categories = [x.string for x in categories]\n",
    "\n",
    "# Total number of Photos\n",
    "tot_n_photos = soup.select('.css-1fepc68 .css-ardur')\n",
    "tot_n_photos = int(tot_n_photos[0].string.split(' ')[1])\n",
    "\n",
    "# Health Score\n",
    "try:\n",
    "    health_score = soup.select('.label-spacing-v2__373c0__PBYkt')\n",
    "    health_score = int(health_score[0].string.split(' ')[0])\n",
    "except:\n",
    "    health_score = None\n",
    "\n",
    "# Covid Updates\n",
    "cov_updates = soup.select('.margin-b1__373c0__1khoT .css-1h1j0y3')\n",
    "cov_updates = [x.string for x in cov_updates]\n",
    "\n",
    "# Street Address\n",
    "st_address = soup.select('.css-1bmgof7 .raw__373c0__3rcx7')\n",
    "st_address = st_address[0].string    \n",
    "\n",
    "#business_info = [business_name, business_url, total_reviews, claimed_status,\n",
    " #                    price_cat, categories, tot_n_photos, health_score, cov_updates, st_address]\n",
    "\n",
    "#     for x in business_info:\n",
    "#         print(x)\n",
    "\n",
    "#### Get the data for multiple review pages ####\n",
    "#### Set the number of review pages that should be scraped ####\n",
    "\n",
    "for i in [2]:\n",
    "\n",
    "    url2 = business_url + '?start=' + str(i * 10)\n",
    "    html = requests.get(url2)\n",
    "    soup = BeautifulSoup(html.content, 'html.parser')\n",
    "\n",
    "    #### Names\n",
    "    names = soup.select('.css-m6anxm .css-166la90')\n",
    "    review_names = []\n",
    "\n",
    "    for name in names:\n",
    "        review_names.append(name.string)\n",
    "\n",
    "    #### Ratings\n",
    "    ratings = soup.select('.margin-b1-5__373c0__2Wblx .overflow--hidden__373c0__2B0kz')\n",
    "\n",
    "    review_ratings = []\n",
    "\n",
    "    for rating in ratings:\n",
    "        review_ratings.append(float(re.sub(' star rating', \n",
    "                                           '',rating.attrs['aria-label'])))\n",
    "    #### Dates\n",
    "    dates = soup.select('.margin-b1-5__373c0__2Wblx .css-e81eai')\n",
    "\n",
    "    review_dates = []\n",
    "\n",
    "    for date in dates:\n",
    "        review_dates.append(date.string)\n",
    "\n",
    "    #### Text\n",
    "    texts = soup.select('.css-n6i4z7 .raw__373c0__3rcx7')\n",
    "    review_texts = []\n",
    "\n",
    "    for text in texts:\n",
    "        review_texts.append(text.get_text())\n",
    "\n",
    "    #### Reviewer's Location\n",
    "    reviewer_loc = soup.select('.arrange-unit-fill__373c0__17z0h .border-color--default__373c0__2oFDT .border-color--default__373c0__2oFDT .border-color--default__373c0__2oFDT .css-n6i4z7')\n",
    "\n",
    "    reviewer_city = []\n",
    "    reviewer_state = []\n",
    "\n",
    "    for loc in reviewer_loc:\n",
    "        if len(loc.get_text().split(',')) == 2:\n",
    "            reviewer_city.append(loc.get_text().split(',')[0])\n",
    "            reviewer_state.append(loc.get_text().split(',')[1])\n",
    "        elif len(loc.get_text().split(',')) == 3:\n",
    "            reviewer_city.append(loc.get_text().split(',')[1])\n",
    "            reviewer_state.append(loc.get_text().split(',')[2])\n",
    "        else:\n",
    "            reviewer_city.append(None)\n",
    "            reviewer_city.append(None)    \n",
    "\n",
    "    #### Votes\n",
    "    votes = soup.select('.arrange-unit-fill__373c0__17z0h .display--inline__373c0__1DbOG .css-1ha1j8d')\n",
    "\n",
    "    useful_votes = []\n",
    "    funny_votes = []\n",
    "    cool_votes = []\n",
    "\n",
    "    for vote in votes:\n",
    "        if vote.get_text().split(' ')[0] == 'Useful':\n",
    "            try:\n",
    "                useful_votes.append(vote.get_text().split(' ')[1])\n",
    "            except:\n",
    "                useful_votes.append(0)\n",
    "        elif vote.get_text().split(' ')[0] == 'Funny':\n",
    "            try:\n",
    "                funny_votes.append(vote.get_text().split(' ')[1])\n",
    "            except:\n",
    "                funny_votes.append(0)\n",
    "        else:\n",
    "            try:\n",
    "                cool_votes.append(vote.get_text().split(' ')[1])\n",
    "            except:\n",
    "                cool_votes.append(0)    \n",
    "    review_vars1 = [review_names, review_ratings, review_dates, \n",
    "                    review_texts, reviewer_city, reviewer_state,\n",
    "                    useful_votes, funny_votes, cool_votes]\n",
    "\n",
    "\n",
    "    #### Getting the inconsistent variables ####\n",
    "    n_photos = []\n",
    "    r_friends = []\n",
    "    r_reviews = []\n",
    "    r_photos = []\n",
    "    elite_tag = []\n",
    "\n",
    "    raw_reviews = soup.select('.review__373c0__13kpL')\n",
    "    for raw_review in raw_reviews:\n",
    "\n",
    "        ### Number of Photos in the review ###\n",
    "        raw_n_photos = raw_review.select('.margin-b0-5__373c0__Fo75u:nth-child(1) .css-166la90')\n",
    "        if len(raw_n_photos) == 0:\n",
    "            n_photos.append(0)\n",
    "        else:\n",
    "            try:\n",
    "                n_photos.append(int(raw_n_photos[0].string.split()[0]))\n",
    "            except:\n",
    "                n_photos.append(0)\n",
    "\n",
    "        ### Profile tags (# Friends, # Reviews, # Pictures) ###   \n",
    "        raw_tags = raw_review.select('.arrange-unit-fill__373c0__17z0h .border-color--default__373c0__2oFDT .border-color--default__373c0__2oFDT .border-color--default__373c0__2oFDT .border-color--default__373c0__2oFDT .border-color--default__373c0__2oFDT .border-color--default__373c0__2oFDT .css-1dgkz3l')\n",
    "        r_friends.append(int(raw_tags[0].string))\n",
    "        r_reviews.append(int(raw_tags[1].string))\n",
    "        try:\n",
    "            r_photos.append(int(raw_tags[2].string))\n",
    "        except:\n",
    "            r_photos.append(0)\n",
    "\n",
    "        ### The \"Yelp Elite Squad\" tag ###\n",
    "        raw_elite_tag = raw_review.select('.css-1hx6l2b')\n",
    "        if len(raw_elite_tag) == 0:\n",
    "            elite_tag.append(0)\n",
    "        else:\n",
    "            elite_tag.append(1)\n",
    "\n",
    "    #print(n_photos, r_photos, elite_tag) \n",
    "\n",
    "    #### Combining all variables into 1 dataset ####\n",
    "\n",
    "    business_names = [business_name] * len(review_names)\n",
    "    business_urls = [business_url] * len(review_names)\n",
    "    total_reviews_all = [total_reviews] * len(review_names)\n",
    "    claimed_status_all = [claimed_status] * len(review_names)\n",
    "    price_cat_all = [price_cat] * len(review_names)\n",
    "    categories_all = [categories] * len(review_names)\n",
    "    tot_n_photos_all = [tot_n_photos] * len(review_names)\n",
    "    health_score_all = [health_score] * len(review_names)\n",
    "    cov_updates_all = [cov_updates] * len(review_names)\n",
    "    st_address_all = [st_address] * len(review_names)\n",
    "\n",
    "    review_data = list(zip(business_names, business_urls, total_reviews_all, \n",
    "                           claimed_status_all,price_cat_all, categories_all, \n",
    "                           tot_n_photos_all, health_score_all, cov_updates_all,\n",
    "                           st_address_all, review_names, review_ratings, review_dates, \n",
    "                           review_texts, n_photos, r_friends, r_reviews, \n",
    "                           r_photos, elite_tag))\n",
    "\n",
    "    df_reviews = pd.DataFrame(data=review_data,\n",
    "                              columns = ['name_business', 'url', 'total_reviews', \n",
    "                                         'claimed_status','price_cat', 'categories', \n",
    "                                         'tot_n_photos', 'health_score', 'cov_updates', \n",
    "                                         'st_address','username', 'rating', \n",
    "                                         'date_review', 'text', 'n_photos',\n",
    "                                         'r_friends', 'r_reviews', 'r_photos',\n",
    "                                         'elite_tag'])\n",
    "\n",
    "    with open('cleantest2.csv', 'a', newline='', encoding='utf-8') as f:\n",
    "        df_reviews.to_csv(f, index=False, header=False)\n",
    "\n",
    "    #print('{} review page {} has been scraped'.format(row[1]['name'], str(i)))\n",
    "    time.sleep(randint(2, 10))\n",
    "\n",
    "\n",
    "#print('{} has been scraped'.format(row[1]['name']))    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After getting blocked on Yelp, we can continue the scraper where it left off by looking at the last business in the csv, and continuing from the next business in the master list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews = pd.read_csv('memphis_reviews.csv', header=None, encoding='utf-8')\n",
    "idx = df.index[df['name'] == df_reviews.iloc[-1][0]]\n",
    "df_continue = df[(idx[0]+1):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chez Philippe https://www.yelp.com/biz/chez-philippe-memphis\n",
      "Error in stage 1\n",
      "Flying Fish https://www.yelp.com/biz/flying-fish-memphis-2\n",
      "Error in stage 1\n",
      "Pete & Sam’s Restaurant https://www.yelp.com/biz/pete-and-sams-restaurant-memphis\n",
      "Error in stage 1\n",
      "Orange Mound Grill https://www.yelp.com/biz/orange-mound-grill-memphis\n",
      "Error in stage 1\n",
      "Huey’s - Midtown https://www.yelp.com/biz/hueys-midtown-memphis-4\n",
      "Error in stage 1\n",
      "Dino’s Grill https://www.yelp.com/biz/dinos-grill-memphis\n",
      "Error in stage 1\n",
      "Ciao Bella Italian Grill and Bar https://www.yelp.com/biz/ciao-bella-italian-grill-and-bar-memphis\n",
      "Error in stage 1\n",
      "Freddy’s Frozen Custard & Steakburgers https://www.yelp.com/biz/freddys-frozen-custard-and-steakburgers-memphis-5\n",
      "Error in stage 1\n",
      "Bala’s Bistro https://www.yelp.com/biz/balas-bistro-memphis-2\n",
      "Error in stage 1\n",
      "Venice Kitchen https://www.yelp.com/biz/venice-kitchen-memphis-2\n",
      "Error in stage 1\n",
      "Marciano https://www.yelp.com/biz/marciano-memphis\n",
      "Error in stage 1\n",
      "Dixie Queen https://www.yelp.com/biz/dixie-queen-memphis-17\n",
      "Error in stage 1\n",
      "La Baguette French Bread and Pastry Shop https://www.yelp.com/biz/la-baguette-french-bread-and-pastry-shop-memphis\n",
      "Error in stage 1\n",
      "Gus’s World Famous Fried Chicken https://www.yelp.com/biz/guss-world-famous-fried-chicken-memphis-2\n",
      "Error in stage 1\n",
      "Island Paradise Takeout https://www.yelp.com/biz/island-paradise-takeout-memphis\n",
      "Error in stage 1\n",
      "Ball Hoggerz BBQ https://www.yelp.com/biz/ball-hoggerz-bbq-memphis\n",
      "Error in stage 1\n",
      "The Bar-B-Q Shop https://www.yelp.com/biz/the-bar-b-q-shop-memphis-6\n",
      "Error in stage 1\n",
      "Fino’s from the Hill https://www.yelp.com/biz/finos-from-the-hill-memphis-2\n",
      "Error in stage 1\n",
      "Napa Cafe https://www.yelp.com/biz/napa-cafe-memphis\n",
      "Error in stage 1\n",
      "McEwen’s Memphis - Temp. CLOSED https://www.yelp.com/biz/mcewens-memphis-memphis\n",
      "Error in stage 1\n",
      "Tropical Smoothie Cafe - Opening Soon https://www.yelp.com/biz/tropical-smoothie-cafe-opening-soon-memphis-2\n",
      "Error in stage 1\n",
      "DeeO’s Seafood https://www.yelp.com/biz/deeos-seafood-memphis-2\n",
      "Error in stage 1\n",
      "Pho Saigon https://www.yelp.com/biz/pho-saigon-memphis\n",
      "Error in stage 1\n",
      "Edge Alley https://www.yelp.com/biz/edge-alley-memphis-2\n",
      "Error in stage 1\n",
      "The Vault https://www.yelp.com/biz/the-vault-memphis-6\n",
      "Error in stage 1\n",
      "Cafe Olé https://www.yelp.com/biz/cafe-ol%C3%A9-memphis-3\n",
      "Error in stage 1\n",
      "Mortimer’s Restaurant https://www.yelp.com/biz/mortimers-restaurant-memphis\n",
      "Error in stage 1\n",
      "Ubee’s https://www.yelp.com/biz/ubees-memphis\n",
      "Error in stage 1\n",
      "Suga Shack https://www.yelp.com/biz/suga-shack-memphis\n",
      "Error in stage 1\n",
      "Bardog Tavern https://www.yelp.com/biz/bardog-tavern-memphis\n",
      "Error in stage 1\n",
      "Abyssinia Ethiopian Restaurant https://www.yelp.com/biz/abyssinia-ethiopian-restaurant-memphis-3\n",
      "Error in stage 1\n",
      "Buckley’s Restaurant East https://www.yelp.com/biz/buckleys-restaurant-east-memphis\n",
      "Error in stage 1\n",
      "Bluff City Coffee https://www.yelp.com/biz/bluff-city-coffee-memphis-2\n",
      "Error in stage 1\n",
      "Elena’s Taco Shop https://www.yelp.com/biz/elenas-taco-shop-memphis\n",
      "Error in stage 1\n",
      "The Backlot Sandwich Shop https://www.yelp.com/biz/the-backlot-sandwich-shop-memphis\n",
      "Error in stage 1\n",
      "Cheffie’s Cafe https://www.yelp.com/biz/cheffies-cafe-memphis\n",
      "Error in stage 1\n",
      "Pontotoc https://www.yelp.com/biz/pontotoc-memphis\n",
      "Error in stage 1\n",
      "Mulan Asian Bistro https://www.yelp.com/biz/mulan-asian-bistro-memphis\n",
      "Error in stage 1\n",
      "Garibaldi’s Pizza https://www.yelp.com/biz/garibaldis-pizza-memphis-2\n",
      "Error in stage 1\n",
      "Caritas Village https://www.yelp.com/biz/caritas-village-memphis\n",
      "Error in stage 1\n",
      "River Oaks Restaurant https://www.yelp.com/biz/river-oaks-restaurant-memphis\n",
      "Error in stage 1\n",
      "Coletta’s Italian Restaurant https://www.yelp.com/biz/colettas-italian-restaurant-memphis\n",
      "Error in stage 1\n",
      "Carolina Watershed Memphis https://www.yelp.com/biz/carolina-watershed-memphis-memphis\n",
      "Error in stage 1\n",
      "Celtic Crossing Irish Pub & Restaurant https://www.yelp.com/biz/celtic-crossing-irish-pub-and-restaurant-memphis\n",
      "Error in stage 1\n",
      "Ruth’s Chris Steak House https://www.yelp.com/biz/ruths-chris-steak-house-memphis\n",
      "Error in stage 1\n",
      "Paulette’s Restaurant https://www.yelp.com/biz/paulettes-restaurant-memphis\n",
      "Error in stage 1\n",
      "Agavos Cocina & Tequila https://www.yelp.com/biz/agavos-cocina-and-tequila-memphis\n",
      "Error in stage 1\n",
      "Southall Cafe https://www.yelp.com/biz/southall-cafe-memphis-3\n",
      "Error in stage 1\n",
      "South Memphis Grocery https://www.yelp.com/biz/south-memphis-grocery-memphis\n",
      "Error in stage 1\n",
      "Loflin Yard https://www.yelp.com/biz/loflin-yard-memphis\n",
      "Error in stage 1\n",
      "Cocina Mexicana https://www.yelp.com/biz/cocina-mexicana-memphis\n",
      "Error in stage 1\n",
      "Raw Girls https://www.yelp.com/biz/raw-girls-memphis-3\n",
      "Error in stage 1\n",
      "Texas de Brazil https://www.yelp.com/biz/texas-de-brazil-memphis-3\n",
      "Error in stage 1\n",
      "Picosos https://www.yelp.com/biz/picosos-memphis\n",
      "Error in stage 1\n",
      "The Woman’s Exchange of Memphis - Tea Room https://www.yelp.com/biz/the-womans-exchange-of-memphis-tea-room-memphis\n",
      "Error in stage 1\n",
      "Fleming’s Prime Steakhouse & Wine Bar https://www.yelp.com/biz/fleming-s-prime-steakhouse-and-wine-bar-memphis\n",
      "Error in stage 1\n",
      "Willie Mae’s Southern Soul Cooking https://www.yelp.com/biz/willie-maes-southern-soul-cooking-memphis-2\n",
      "Error in stage 1\n",
      "Marlowe’s Ribs & Restaurant https://www.yelp.com/biz/marlowes-ribs-and-restaurant-memphis-2\n",
      "Error in stage 1\n",
      "Seasons 52 https://www.yelp.com/biz/seasons-52-memphis-3\n",
      "Error in stage 1\n",
      "Redhook Cajun Seafood & Bar https://www.yelp.com/biz/redhook-cajun-seafood-and-bar-memphis\n",
      "Error in stage 1\n",
      "Mesquite Chop House https://www.yelp.com/biz/mesquite-chop-house-memphis\n",
      "Error in stage 1\n",
      "Queen Of Sheba https://www.yelp.com/biz/queen-of-sheba-memphis\n",
      "Error in stage 1\n",
      "Peggy’s Heavenly Home Cooking https://www.yelp.com/biz/peggys-heavenly-home-cooking-memphis-3\n",
      "Error in stage 1\n"
     ]
    }
   ],
   "source": [
    "for row in df_small.iterrows():\n",
    "    try:\n",
    "        #### Get the business name and url ####\n",
    "        business_name = row[1]['name']\n",
    "        business_url = row[1]['url']\n",
    "        print(business_name, business_url)\n",
    "\n",
    "        #### Request the page ####\n",
    "        html = requests.get(business_url)\n",
    "        soup = BeautifulSoup(html.content, 'html.parser')\n",
    "        \n",
    "        #### Average Rating ####\n",
    "        try:\n",
    "            mean_rating = soup.select('.i-stars--large-5__373c0__1GcGD')\n",
    "            mean_rating = float(mean_rating[0].attrs['aria-label'].split(' ')[0])\n",
    "        except:\n",
    "            try:\n",
    "                mean_rating = soup.select('.i-stars--large-4-half__373c0__2lYkD')\n",
    "                mean_rating = float(mean_rating[0].attrs['aria-label'].split(' ')[0])\n",
    "            except:\n",
    "                try:\n",
    "                    mean_rating = soup.select('.i-stars--large-4__373c0__1d6HV')\n",
    "                    mean_rating = float(mean_rating[0].attrs['aria-label'].split(' ')[0])\n",
    "                except:\n",
    "                    try:\n",
    "                        mean_rating = soup.select('.i-stars--large-3-half__373c0__2z4jR')\n",
    "                        mean_rating = float(mean_rating[0].attrs['aria-label'].split(' ')[0])\n",
    "                    except:\n",
    "                        try: \n",
    "                            mean_rating = soup.select('.i-stars--large-3__373c0__3_Jon')\n",
    "                            mean_rating = float(mean_rating[0].attrs['aria-label'].split(' ')[0])\n",
    "                        except:\n",
    "                            mean_rating = 0\n",
    "\n",
    "\n",
    "        # Total Number of Reviews\n",
    "        total_reviews = soup.select('.arrange-unit-fill__373c0__17z0h.nowrap__373c0__1_N1j .css-bq71j2')\n",
    "        total_reviews = int(total_reviews[0].string.split(' ')[0])\n",
    "\n",
    "        # Claimed Status\n",
    "        try:\n",
    "            claimed_status = soup.select('.css-1xxismk .css-166la90')[0].string\n",
    "            claimed_status = 0\n",
    "        except:\n",
    "            claimed_status = 1\n",
    "\n",
    "        # Price Category\n",
    "        try:\n",
    "            price_cat = soup.select('.margin-r1__373c0__zyKmV+ .border-color--default__373c0__2oFDT .css-1xxismk')\n",
    "            price_cat = len((price_cat[0].text).strip())\n",
    "        except:\n",
    "            price_cat = None\n",
    "            \n",
    "        # Restaurant Categories\n",
    "        categories = soup.select('.css-bq71j2 .css-166la90')\n",
    "        categories = [x.string for x in categories]\n",
    "\n",
    "        # Total number of Photos\n",
    "        try:\n",
    "            tot_n_photos = soup.select('.css-1fepc68 .css-ardur')\n",
    "            tot_n_photos = int(tot_n_photos[0].string.split(' ')[1])\n",
    "        except:\n",
    "            tot_n_photos = 0\n",
    "            \n",
    "        # Health Score\n",
    "        try:\n",
    "            health_score = soup.select('.label-spacing-v2__373c0__PBYkt')\n",
    "            health_score = int(health_score[0].string.split(' ')[0])\n",
    "        except:\n",
    "            health_score = None\n",
    "            \n",
    "        # Covid Updates\n",
    "        cov_updates = soup.select('.margin-b1__373c0__1khoT .css-1h1j0y3')\n",
    "        cov_updates = [x.string for x in cov_updates]\n",
    "\n",
    "        # Street Address\n",
    "        st_address = soup.select('.css-1bmgof7 .raw__373c0__3rcx7')\n",
    "        st_address = st_address[0].string    \n",
    "\n",
    "        business_info = [business_name, business_url, total_reviews, claimed_status,\n",
    "                             price_cat, categories, tot_n_photos, health_score, cov_updates, st_address]\n",
    "\n",
    "        #### Get the data for multiple review pages ####\n",
    "        #### Set the number of review pages that should be scraped ####\n",
    "\n",
    "        for i in range(2):\n",
    "            try:\n",
    "                url2 = business_url + '?start=' + str(i * 10)\n",
    "                html = requests.get(url2)\n",
    "                soup = BeautifulSoup(html.content, 'html.parser')\n",
    "\n",
    "                #### Names\n",
    "                names = soup.select('.css-m6anxm .css-166la90')\n",
    "                review_names = []\n",
    "\n",
    "                for name in names:\n",
    "                    review_names.append(name.string)\n",
    "\n",
    "                #### Ratings\n",
    "                ratings = soup.select('.margin-b1-5__373c0__2Wblx .overflow--hidden__373c0__2B0kz')\n",
    "\n",
    "                review_ratings = []\n",
    "\n",
    "                for rating in ratings:\n",
    "                    review_ratings.append(float(re.sub(' star rating', \n",
    "                                                       '',rating.attrs['aria-label'])))\n",
    "                #### Dates\n",
    "                dates = soup.select('.margin-b1-5__373c0__2Wblx .css-e81eai')\n",
    "\n",
    "                review_dates = []\n",
    "\n",
    "                for date in dates:\n",
    "                    review_dates.append(date.string)\n",
    "\n",
    "                #### Text\n",
    "                texts = soup.select('.css-n6i4z7 .raw__373c0__3rcx7')\n",
    "                review_texts = []\n",
    "\n",
    "                for text in texts:\n",
    "                    review_texts.append(text.get_text())\n",
    "\n",
    "                #### Reviewer's Location\n",
    "                reviewer_loc = soup.select('.arrange-unit-fill__373c0__17z0h .border-color--default__373c0__2oFDT .border-color--default__373c0__2oFDT .border-color--default__373c0__2oFDT .css-n6i4z7')\n",
    "\n",
    "                reviewer_city = []\n",
    "                reviewer_state = []\n",
    "\n",
    "                for loc in reviewer_loc:\n",
    "                    if len(loc.get_text().split(',')) == 2:\n",
    "                        reviewer_city.append(loc.get_text().split(',')[0])\n",
    "                        reviewer_state.append(loc.get_text().split(',')[1])\n",
    "                    elif len(loc.get_text().split(',')) == 3:\n",
    "                        reviewer_city.append(loc.get_text().split(',')[1])\n",
    "                        reviewer_state.append(loc.get_text().split(',')[2])\n",
    "                    else:\n",
    "                        reviewer_city.append(None)\n",
    "                        reviewer_city.append(None)    \n",
    "\n",
    "                #### Votes\n",
    "                votes = soup.select('.arrange-unit-fill__373c0__17z0h .display--inline__373c0__1DbOG .css-1ha1j8d')\n",
    "\n",
    "                useful_votes = []\n",
    "                funny_votes = []\n",
    "                cool_votes = []\n",
    "\n",
    "                for vote in votes:\n",
    "                    if vote.get_text().split(' ')[0] == 'Useful':\n",
    "                        try:\n",
    "                            useful_votes.append(vote.get_text().split(' ')[1])\n",
    "                        except:\n",
    "                            useful_votes.append(0)\n",
    "                    elif vote.get_text().split(' ')[0] == 'Funny':\n",
    "                        try:\n",
    "                            funny_votes.append(vote.get_text().split(' ')[1])\n",
    "                        except:\n",
    "                            funny_votes.append(0)\n",
    "                    else:\n",
    "                        try:\n",
    "                            cool_votes.append(vote.get_text().split(' ')[1])\n",
    "                        except:\n",
    "                            cool_votes.append(0)    \n",
    "                review_vars1 = [review_names, review_ratings, review_dates, \n",
    "                                review_texts, reviewer_city, reviewer_state,\n",
    "                                useful_votes, funny_votes, cool_votes]\n",
    "\n",
    "\n",
    "                #### Getting the inconsistent variables ####\n",
    "                n_photos = []\n",
    "                r_friends = []\n",
    "                r_reviews = []\n",
    "                r_photos = []\n",
    "                elite_tag = []\n",
    "\n",
    "                raw_reviews = soup.select('.review__373c0__13kpL')\n",
    "                for raw_review in raw_reviews:\n",
    "\n",
    "                    ### Number of Photos in the review ###\n",
    "                    raw_n_photos = raw_review.select('.margin-b0-5__373c0__Fo75u:nth-child(1) .css-166la90')\n",
    "                    if len(raw_n_photos) == 0:\n",
    "                        n_photos.append(0)\n",
    "                    else:\n",
    "                        try:\n",
    "                            n_photos.append(int(raw_n_photos[0].string.split()[0]))\n",
    "                        except:\n",
    "                            n_photos.append(0)\n",
    "\n",
    "                    ### Profile tags (# Friends, # Reviews, # Pictures) ###   \n",
    "                    raw_tags = raw_review.select('.arrange-unit-fill__373c0__17z0h .border-color--default__373c0__2oFDT .border-color--default__373c0__2oFDT .border-color--default__373c0__2oFDT .border-color--default__373c0__2oFDT .border-color--default__373c0__2oFDT .border-color--default__373c0__2oFDT .css-1dgkz3l')\n",
    "                    r_friends.append(int(raw_tags[0].string))\n",
    "                    r_reviews.append(int(raw_tags[1].string))\n",
    "                    try:\n",
    "                        r_photos.append(int(raw_tags[2].string))\n",
    "                    except:\n",
    "                        r_photos.append(0)\n",
    "\n",
    "                    ### The \"Yelp Elite Squad\" tag ###\n",
    "                    raw_elite_tag = raw_review.select('.css-1hx6l2b')\n",
    "                    if len(raw_elite_tag) == 0:\n",
    "                        elite_tag.append(0)\n",
    "                    else:\n",
    "                        elite_tag.append(1)\n",
    "\n",
    "                #### Combining all variables into 1 dataset ####\n",
    "\n",
    "                business_names = [business_name] * len(review_names)\n",
    "                business_urls = [business_url] * len(review_names)\n",
    "                mean_ratings = [mean_rating] * len(review_names)\n",
    "                total_reviews_all = [total_reviews] * len(review_names)\n",
    "                claimed_status_all = [claimed_status] * len(review_names)\n",
    "                price_cat_all = [price_cat] * len(review_names)\n",
    "                categories_all = [categories] * len(review_names)\n",
    "                tot_n_photos_all = [tot_n_photos] * len(review_names)\n",
    "                health_score_all = [health_score] * len(review_names)\n",
    "                cov_updates_all = [cov_updates] * len(review_names)\n",
    "                st_address_all = [st_address] * len(review_names)\n",
    "\n",
    "                review_data = list(zip(business_names, business_urls, mean_ratings, total_reviews_all, \n",
    "                                       claimed_status_all,price_cat_all, categories_all, \n",
    "                                       tot_n_photos_all, health_score_all, cov_updates_all,\n",
    "                                       st_address_all, review_names, review_ratings, review_dates, \n",
    "                                       review_texts, n_photos, r_friends, r_reviews, \n",
    "                                       r_photos, elite_tag))\n",
    "\n",
    "                df_reviews = pd.DataFrame(data=review_data,\n",
    "                                          columns = ['name_business', 'url', 'mean_rating', 'total_reviews', \n",
    "                                                     'claimed_status','price_cat', 'categories', \n",
    "                                                     'tot_n_photos', 'health_score', 'cov_updates', \n",
    "                                                     'st_address','username', 'rating', \n",
    "                                                     'date_review', 'text', 'n_photos',\n",
    "                                                     'r_friends', 'r_reviews', 'r_photos',\n",
    "                                                     'elite_tag'])\n",
    "\n",
    "                with open('memphis_reviews4.csv', 'a', newline='', encoding='utf-8') as f:\n",
    "                    df_reviews.to_csv(f, index=False, header=False)\n",
    "\n",
    "                print('{} review page {} has been scraped'.format(row[1]['name'], str(i)))\n",
    "                time.sleep(randint(4, 12))\n",
    "                \n",
    "            except:\n",
    "                print('Error in stage 2')\n",
    "\n",
    "        print('{} has been scraped'.format(row[1]['name']))    \n",
    "        time.sleep(randint(4, 12))\n",
    "    \n",
    "    except:\n",
    "        print('Error in stage 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}